%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AI3603 Billiards Project Report
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}

\IEEEoverridecommandlockouts
\overrideIEEEmargins

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{\LARGE \bf
Exploring AI Approaches for Billiards: \\
From Physics Simulation to Imitation Learning
}

\author{Abigail Jin% <-this % stops a space
\thanks{Department of Computer Science and Engineering, Shanghai Jiao Tong University}%
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

This report documents my exploration of various AI approaches for the billiards game challenge in AI3603 course. I investigated three main directions: (1) physics-based simulation agent using geometric calculations and physics engine verification, (2) reinforcement learning with Soft Actor-Critic (SAC) algorithm, and (3) imitation learning from Monte Carlo Tree Search (MCTS) expert. While the physics-based approach achieved 60\% win rate against BasicAgent, and other learning-based methods faced significant challenges, these experiments provide valuable insights into why search-based methods like MCTS remain superior for this task. The final MCTS solution, developed collaboratively with my teammate, achieved 80.6\% win rate against BasicAgentPro.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Billiards presents a challenging testbed for AI decision-making due to its continuous action space, complex physics dynamics, and long-horizon planning requirements. In this project, I explored multiple AI paradigms to develop an intelligent billiards agent:

\begin{itemize}
    \item \textbf{Physics-based Agent}: Using geometric calculations and physics simulation to evaluate shot quality
    \item \textbf{Reinforcement Learning}: Training an agent via Soft Actor-Critic (SAC) with curriculum learning
    \item \textbf{Imitation Learning}: Learning from MCTS expert demonstrations through behavior cloning
\end{itemize}

This report presents my methodology, experimental results, and lessons learned from each approach. While individual approaches faced limitations, the exploration informed our team's final MCTS-based solution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{PHYSICS-BASED AGENT}

\subsection{Motivation}

Before exploring learning-based methods, I developed a physics-based agent to establish a baseline understanding of the problem. The core insight is that billiards involves precise geometric relationships between cue ball, target ball, and pocket positions.

\subsection{Methodology}

The physics-based agent follows a multi-stage decision process:

\textbf{1. Geometric Calculation}: For each target ball and pocket combination, compute the ``ghost ball'' position—where the cue ball must be to pocket the target:
\begin{equation}
\vec{p}_{ghost} = \vec{p}_{target} - 2R \cdot \hat{d}_{target \rightarrow pocket}
\end{equation}
where $R$ is the ball radius and $\hat{d}$ is the unit direction vector.

\textbf{2. Cut Angle Evaluation}: Calculate the cut angle $\theta_{cut}$ between approach direction and target direction. Shots with $\theta_{cut} > 55°$ are rejected due to noise sensitivity.

\textbf{3. Path Obstruction Check}: Verify that paths from cue to ghost ball and from target to pocket are clear, with safety margins accounting for execution noise.

\textbf{4. Danger Assessment}: Identify dangerous balls (8-ball when own balls remain, opponent balls) and penalize shots that risk pocketing them.

\textbf{5. Physics Simulation Verification}: Use the pooltool physics engine to simulate candidate shots with noise perturbations, selecting the most robust action.

\subsection{Results}

\begin{table}[h]
\caption{Physics-based Agent vs BasicAgent (200 games)}
\label{tab:physics_results}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Agent} & \textbf{Wins} & \textbf{Win Rate} \\
\hline
Physics Agent & 120 & 60.0\% \\
BasicAgent & 64 & 32.0\% \\
Draw & 16 & 8.0\% \\
\hline
\end{tabular}
\end{center}
\end{table}

The physics-based agent achieved a 60\% win rate against BasicAgent, with an average of 39.1 shots per game. While promising, this approach has inherent limitations:

\begin{itemize}
    \item Greedy decision-making without long-term planning
    \item No learning from experience
    \item Sensitivity to complex multi-ball configurations
\end{itemize}

\subsection{Continuation by Teammate}

This physics-based foundation was later extended by my teammate using Monte Carlo Tree Search (MCTS), which addresses the planning limitation by simulating future states. The MCTS approach achieved significantly better results (see Section \ref{sec:comparison}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{REINFORCEMENT LEARNING WITH SAC}

\subsection{Motivation}

Reinforcement learning offers the potential to discover optimal strategies through trial-and-error interaction with the environment, without requiring expert demonstrations.

\subsection{Architecture}

I implemented a Soft Actor-Critic (SAC) \cite{haarnoja2018soft} agent with the following components:

\textbf{State Encoder}: A 53-dimensional feature vector encoding:
\begin{itemize}
    \item Cue ball position and state (3 dims)
    \item Own balls sorted by distance (7 × 3 = 21 dims)
    \item Opponent balls sorted by distance (7 × 3 = 21 dims)
    \item 8-ball position and state (3 dims)
    \item Global statistics (5 dims)
\end{itemize}

\textbf{Actor Network}: Gaussian policy outputting mean and variance for 5 continuous action parameters (V0, $\phi$, $\theta$, a, b).

\textbf{Critic Network}: Twin Q-networks to mitigate overestimation bias.

\subsection{Reward Design}

\begin{equation}
r = r_{pocket} + r_{turn} + r_{foul} + r_{defense}
\end{equation}

where:
\begin{itemize}
    \item $r_{pocket} = \frac{n_{own}}{n_{remaining}} \times 100$ for pocketing own balls
    \item $r_{turn} = +10$ for keeping turn, $-5$ for losing turn
    \item $r_{foul} = -20$ to $-25$ for various fouls
    \item $r_{defense}$: delayed reward when opponent fails
\end{itemize}

\subsection{Training Strategy}

I employed curriculum learning with three stages:
\begin{enumerate}
    \item \textbf{Foundation}: 15k episodes vs BasicAgent
    \item \textbf{Intermediate}: 25k episodes vs mixed opponents
    \item \textbf{Advanced}: 30k episodes including self-play
\end{enumerate}

\subsection{Challenges and Failure Analysis}

Despite extensive implementation effort, SAC training faced fundamental challenges:

\textbf{1. Sparse and Delayed Rewards}: Billiards provides feedback only after physics simulation completes, making credit assignment difficult for the 5-dimensional action space.

\textbf{2. High-Precision Requirements}: Small errors in action parameters (especially $\phi$ angle) lead to completely different outcomes, creating a highly non-smooth reward landscape.

\textbf{3. Exploration Inefficiency}: Random exploration in continuous action space rarely produces successful pockets, providing no learning signal.

The final commit message ``RL is horrifying'' (commit \texttt{3008e3b}) summarizes the experience. After significant debugging of parallel training, reward shaping, and network architectures, the agent failed to learn meaningful policies.

\subsection{Lessons Learned}

This negative result provides valuable insight: for tasks requiring high-precision continuous control with delayed feedback, end-to-end RL may require prohibitive sample complexity. Search-based methods that can evaluate outcomes through simulation are more appropriate.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{IMITATION LEARNING}

\subsection{Motivation}

Given the RL challenges, I explored imitation learning to leverage the strong MCTS expert developed by my teammate. The goal was to distill MCTS's search capability into a fast neural network policy.

\subsection{Data Collection}

I collected 1,787,130 state-action pairs from the long\_mcts agent (400 simulations, 64 candidates) playing against BasicAgent:

\textbf{State Features (80 dimensions)}:
\begin{itemize}
    \item Cue ball: position (x, y) + pocketed flag (3 dims)
    \item 15 balls: position + pocketed flag (45 dims)
    \item Target ball mask (15 dims)
    \item 6 pocket positions (12 dims)
    \item Game statistics (5 dims)
\end{itemize}

\textbf{Action Labels (6 dimensions)}:
\begin{itemize}
    \item V0 (normalized): initial velocity
    \item $\sin(\phi), \cos(\phi)$: horizontal angle encoding
    \item $\theta$ (normalized): vertical angle
    \item a, b: spin parameters
\end{itemize}

\subsection{Network Architectures}

I experimented with multiple architectures:

\textbf{1. MLP Networks}: Simple feedforward networks with varying capacities (8M to 26M parameters).

\textbf{2. Transformer-based}: Ball embeddings with multi-head attention, treating the problem as set prediction.

\textbf{3. Discrete $\phi$ Classification}: Converting the angle regression to 72-bin classification to handle multi-modal actions.

\textbf{4. Equivariant Network}: Exploiting the permutation symmetry of balls using Set Attention.

\subsection{Data Augmentation}

I applied geometric symmetry transformations to increase training data 4×:
\begin{itemize}
    \item Original data
    \item X-axis mirror: $(y \rightarrow 1-y, \phi \rightarrow -\phi)$
    \item Y-axis mirror: $(x \rightarrow 1-x, \phi \rightarrow 180°-\phi)$
    \item 180° rotation: $(x \rightarrow 1-x, y \rightarrow 1-y, \phi \rightarrow \phi+180°)$
\end{itemize}

\subsection{Training Results}

\begin{table}[h]
\caption{Imitation Learning Training Results}
\label{tab:il_results}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Model} & \textbf{Params} & \textbf{Val Loss} & \textbf{Epoch} \\
\hline
Large MLP & 26M & 0.547 & 40 \\
Small MLP & 8M & 0.663 & 47 \\
Aug Small & 8M & 0.636 & 106 \\
Discrete 72bins & 26M & 5.55 & 1 \\
Equivariant & 26M & 2.97 & 25 \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Challenges and Analysis}

\textbf{1. Multi-modal Action Distribution}: The MCTS expert may choose different but equally good shots for similar states. With MSE loss, the network learns to average these actions, producing suboptimal predictions.

\textbf{2. Overfitting}: Despite augmentation and regularization, models showed clear train-validation gaps, indicating insufficient coverage of the state space.

\textbf{3. Angle Accuracy}: The critical $\phi$ angle showed only 15-16\% accuracy in discrete classification, far below what's needed for reliable pocketing.

\subsection{Lessons Learned}

Imitation learning struggles with multi-modal expert behavior. While the network learns general patterns, it cannot capture the precise decision logic of MCTS. Alternative approaches like:
\begin{itemize}
    \item DAgger \cite{ross2011reduction} for distribution shift
    \item Conditional behavior cloning
    \item Mixture density networks
\end{itemize}
might help but require significant additional complexity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{COMPARATIVE ANALYSIS}
\label{sec:comparison}

\begin{table}[h]
\caption{All Agents vs BasicAgentPro}
\label{tab:comparison}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Agent} & \textbf{Win Rate} & \textbf{Avg Shots} \\
\hline
Physics-based & $\sim$40\%* & 39+ \\
SAC (RL) & Failed & - \\
Imitation Learning & Not tested & - \\
\hline
merge\_basic (MCTS) & 56.2\% & - \\
future (MCTS) & 58.2\% & 19.2 \\
\textbf{long\_mcts (MCTS)} & \textbf{80.6\%} & \textbf{15.6} \\
\hline
\end{tabular}
\end{center}
\footnotesize{*Estimated from BasicAgent results}
\end{table}

The MCTS-based agents, developed by my teammate, significantly outperform all my learning-based approaches. Key advantages of MCTS include:

\begin{itemize}
    \item \textbf{Look-ahead Planning}: Evaluates future consequences of actions
    \item \textbf{Robustness}: Tests actions with noise perturbations
    \item \textbf{No Training Required}: Works directly from physics simulation
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CONCLUSION}

This project explored three AI paradigms for billiards:

\begin{enumerate}
    \item \textbf{Physics Simulation}: Provided baseline understanding; 60\% vs BasicAgent
    \item \textbf{Reinforcement Learning}: Failed due to sparse rewards and exploration challenges
    \item \textbf{Imitation Learning}: Limited by multi-modal actions and distribution mismatch
\end{enumerate}

The key insight is that for high-precision physics tasks with available simulators, search-based methods like MCTS are more effective than learning-based approaches. The final solution achieving 80.6\% win rate against BasicAgentPro was developed collaboratively, with my teammate focusing on MCTS optimization while I explored alternative paradigms.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{CONTRIBUTIONS}

\textbf{My contributions}:
\begin{itemize}
    \item Physics-based agent design and implementation
    \item SAC reinforcement learning framework
    \item Imitation learning data collection (1.78M samples) and training
    \item Parallel evaluation framework for testing
    \item Experimental analysis and documentation
\end{itemize}

\textbf{Teammate's contributions}:
\begin{itemize}
    \item MCTS agent development and optimization
    \item Robustness improvements and future reward design
    \item Final solution achieving 80.6\% win rate
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thebibliography}{99}

\bibitem{haarnoja2018soft} T. Haarnoja et al., ``Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,'' ICML 2018.

\bibitem{ross2011reduction} S. Ross et al., ``A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,'' AISTATS 2011.

\bibitem{silver2016mastering} D. Silver et al., ``Mastering the Game of Go with Deep Neural Networks and Tree Search,'' Nature 2016.

\end{thebibliography}

\end{document}
