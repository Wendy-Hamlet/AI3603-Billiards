%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AI3603 Billiards Project Report
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}

\IEEEoverridecommandlockouts
\overrideIEEEmargins

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}

\title{\LARGE \bf
Learning to Play Billiards: \\
A Journey Through Reinforcement and Imitation Learning
}

\author{Abigail Jin% <-this % stops a space
\thanks{Department of Computer Science and Engineering, Shanghai Jiao Tong University}%
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

This report documents my systematic exploration of learning-based approaches for the billiards AI challenge. Starting from a physics-based baseline (60\% win rate), I developed a complete Soft Actor-Critic (SAC) reinforcement learning framework with curriculum learning, followed by an extensive imitation learning pipeline that collected 1.78 million expert demonstrations and experimented with five neural network architectures. While these learning approaches did not surpass the MCTS expert, the journey revealed fundamental insights about why search-based methods excel in high-precision physics tasks: the multi-modal action distribution problem in imitation learning, and the sparse reward challenge in reinforcement learning. The final MCTS solution by my teammate achieved 80.6\% against BasicAgentPro.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Billiards presents a fascinating challenge for artificial intelligence: a continuous action space with five parameters (velocity, angles, spin), complex physics dynamics, and the need for both tactical precision and strategic planning. This project explores whether modern learning algorithms can master this task.

I investigated three approaches in sequence:
\begin{enumerate}
    \item \textbf{Baseline analysis}: Physics-based agent and opponent robustness study (Section II)
    \item \textbf{Reinforcement learning}: Training from scratch via SAC (Section III)
    \item \textbf{Imitation learning}: Learning from MCTS expert demonstrations (Section IV)
\end{enumerate}

Each approach built upon lessons learned from the previous, ultimately revealing why Monte Carlo Tree Search (MCTS) remains the superior approach for this domain.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{BASELINE AGENTS AND OPPONENT ANALYSIS}

\subsection{Physics-Based Baseline}

Before exploring learning methods, I implemented a physics-based agent using geometric shot calculation and physics engine verification. The agent computes ``ghost ball'' positions for each target-pocket combination, evaluates cut angles, checks path obstructions, and uses pooltool simulation to verify shot feasibility.

This approach achieved 60\% win rate against BasicAgent, establishing a baseline understanding of the problem's requirements: precise angle calculation, obstacle awareness, and robustness to execution noise. This work was later extended by my teammate using MCTS, which achieved significantly better results through look-ahead planning.

\subsection{Opponent Robustness Analysis}

To understand the baseline agents we compete against, I designed a robustness test using a \texttt{RandomAgent} that only makes weak random shots (velocity 0.5--1.5 m/s, random direction). A robust agent should win nearly 100\% against such a trivial opponent that almost never pockets balls.

\begin{table}[h]
\caption{Robustness Test: Baseline Agents vs RandomAgent (1000 games)}
\label{tab:robustness}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Agent} & \textbf{Win Rate} & \textbf{Loss Rate} & \textbf{Avg Shots} \\
\hline
BasicAgent & 78.8\% & 21.2\% & 23.8 \\
BasicAgentPro & 81.7\% & 18.2\% & 23.9 \\
\hline
\end{tabular}
\end{center}
\end{table}

\textbf{Key finding}: Even against a nearly-passive opponent, both agents lose approximately 20\% of games. Since \texttt{RandomAgent} almost never pockets balls intentionally, these losses occur when the baseline agent accidentally:
\begin{enumerate}
    \item Pockets the 8-ball prematurely (before clearing own balls)
    \item Scratches (pockets cue ball) while pocketing the 8-ball
    \item Makes other foul-related errors leading to loss
\end{enumerate}

This 20\% ``self-defeat'' rate reveals a fundamental limitation of greedy optimization approaches: they pursue high-reward pocketing actions without adequately considering game-ending foul risks. This insight motivated our exploration of look-ahead methods (MCTS) and learning-based approaches that could potentially learn safer strategies.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{REINFORCEMENT LEARNING WITH SAC}

\subsection{Motivation and Architecture}

Reinforcement learning offers the promise of discovering optimal strategies through interaction, without requiring expert knowledge. I chose Soft Actor-Critic (SAC) \cite{haarnoja2018soft} for its sample efficiency and stability in continuous action spaces.

\subsubsection{SAC Algorithm Overview}

Algorithm~\ref{alg:sac} outlines the SAC training procedure adapted for billiards.

\begin{algorithm}[h]
\caption{SAC for Billiards with Curriculum Learning}
\label{alg:sac}
\begin{algorithmic}[1]
\STATE Initialize policy $\pi_\theta$, Q-networks $Q_{\phi_1}, Q_{\phi_2}$, target networks $Q_{\bar\phi_1}, Q_{\bar\phi_2}$
\STATE Initialize replay buffer $\mathcal{D}$, temperature $\alpha$
\STATE Set curriculum stage $k \leftarrow 1$
\FOR{episode $= 1$ to $N$}
    \STATE Sample initial state $s_0$ from Stage-$k$ environment
    \FOR{$t = 0$ to $T_{max}$}
        \STATE $a_t \sim \pi_\theta(\cdot|s_t)$ \quad \textit{// Sample action with Gaussian exploration}
        \STATE Execute $a_t$, observe $r_t, s_{t+1}$, done
        \STATE Store $(s_t, a_t, r_t, s_{t+1})$ in $\mathcal{D}$
        \IF{$|\mathcal{D}| > N_{min}$}
            \STATE Sample minibatch from $\mathcal{D}$
            \STATE Update critics: $\phi_i \leftarrow \phi_i - \lambda_Q \nabla_{\phi_i} J_Q$
            \STATE Update actor: $\theta \leftarrow \theta - \lambda_\pi \nabla_\theta J_\pi$
            \STATE Update $\alpha$: $\alpha \leftarrow \alpha - \lambda_\alpha \nabla_\alpha J_\alpha$
            \STATE Soft update targets: $\bar\phi_i \leftarrow \tau\phi_i + (1-\tau)\bar\phi_i$
        \ENDIF
        \IF{done}
            \STATE \textbf{break}
        \ENDIF
    \ENDFOR
    \IF{Stage-$k$ target achieved}
        \STATE Advance to stage $k+1$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{State Representation}

I designed a 53-dimensional state encoding with semantic grouping:

\begin{itemize}
    \item \textbf{Cue ball} (3 dims): position $(x, y)$ and pocketed flag
    \item \textbf{Own balls} (21 dims): 7 balls × 3 features, sorted by distance to cue
    \item \textbf{Opponent balls} (21 dims): 7 balls × 3 features, sorted by distance
    \item \textbf{8-ball} (3 dims): special handling for game-ending ball
    \item \textbf{Global info} (5 dims): remaining counts, turn indicator, game phase
\end{itemize}

The distance-based sorting provides implicit priority hints while maintaining permutation consistency.

\subsubsection{Network Architecture}

\textbf{Actor (Gaussian Policy)}:
\begin{equation}
\pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma_\theta(s)^2)
\end{equation}
Outputs mean and log-variance for 5 action dimensions: $V_0$ (velocity), $\phi$ (horizontal angle), $\theta$ (vertical angle), $a$ and $b$ (spin).

\textbf{Critic (Twin Q-Networks)}:
Two independent Q-networks to mitigate overestimation bias:
\begin{equation}
Q_{target} = r + \gamma \min_{i=1,2} Q_{\phi_i'}(s', a') - \alpha \log \pi(a'|s')
\end{equation}

\subsection{Reward Engineering}

Reward design proved critical. I developed a multi-component reward function:

\begin{equation}
r = r_{pocket} + r_{turn} + r_{foul} + r_{defense} + r_{terminal}
\end{equation}

\textbf{Pocketing reward} uses proportional value to encourage clearing:
\begin{equation}
r_{pocket} = \frac{n_{pocketed}}{n_{remaining}} \times 100
\end{equation}

\textbf{Turn dynamics}: $+10$ for keeping turn, $-5$ for losing it.

\textbf{Foul penalties}: $-20$ (scratch), $-15$ (wrong first contact), $-10$ (no rail), $-25$ (complete miss).

\textbf{Delayed defense reward}: When opponent fails on their turn, retroactively add $+5$ to our previous action, encouraging defensive play.

\textbf{Terminal rewards}: Experimented with $\pm 1000$ (large) and $\pm 100$ (compact). Draw penalty was a critical design variable explored in Section III-E.

\subsection{Curriculum Learning Strategy}

To address the exploration challenge, I implemented three-stage curriculum learning:

\textbf{Stage 1 - Foundation} (15k episodes): Train against BasicAgent only. Target: $>70\%$ win rate. Focus on learning legal shots and basic pocketing.

\textbf{Stage 2 - Intermediate} (25k episodes): Mixed opponents (60\% BasicAgent, 30\% PhysicsAgent, 10\% self-play). Target: $>40\%$ against PhysicsAgent.

\textbf{Stage 3 - Advanced} (30k episodes): Include MCTS opponent (20\% Basic, 30\% Physics, 20\% MCTS, 30\% self-play). Target: $>30\%$ against MCTS.

\subsection{Implementation Challenges}

\subsubsection{Parallel Training}
Single-threaded training was prohibitively slow due to physics simulation. I implemented parallel environment workers with careful handling of:
\begin{itemize}
    \item NumPy/Numba float64 dtype requirements for pooltool compatibility
    \item Process-safe replay buffer updates
    \item Synchronized policy updates across workers
\end{itemize}

\subsubsection{Numerical Stability}
The billiards environment produces occasional extreme values. I added gradient clipping, action bound enforcement, and NaN detection with automatic recovery.

\subsection{Experimental Analysis}

Despite extensive engineering, SAC training failed to produce a competent agent. To understand the failure modes, I conducted three controlled experiments in a simplified 4-ball scenario (1 own ball, 1 enemy ball, 8-ball, cue ball) with self-play training.

\subsubsection{Experiment 1: Large Reward ($\pm$1000)}

Initial training used large terminal rewards ($\pm 1000$ for win/loss). The hypothesis was that stronger reward signals would accelerate learning.

\begin{table}[h]
\caption{Experiment 1: Large Reward Training (200,000 games)}
\label{tab:exp1}
\begin{center}
\begin{tabular}{|r|c|c|c|c|}
\hline
\textbf{Games} & \textbf{A Win} & \textbf{B Win} & \textbf{Draw} & \textbf{Pockets} \\
\hline
800 & 45\% & 46\% & 9\% & 0.53 \\
8,000 & 26\% & 26\% & 48\% & 0.25 \\
200,000 & 25\% & 25\% & \textbf{51\%} & 0.32 \\
\hline
\end{tabular}
\end{center}
\end{table}

\textbf{Result}: Despite 200,000 training games, the agent converged to a ``lazy'' equilibrium with 51\% draw rate. The agent learned to avoid risky shots, preferring to prolong games until the 60-shot limit.

\subsubsection{Experiment 2: Compact Reward ($\pm$100)}

Reduced terminal rewards to $\pm 100$, hypothesizing that smaller magnitudes might stabilize training.

\begin{table}[h]
\caption{Experiment 2: Compact Reward Training (20,000 games)}
\label{tab:exp2}
\begin{center}
\begin{tabular}{|r|c|c|c|c|}
\hline
\textbf{Games} & \textbf{A Win} & \textbf{B Win} & \textbf{Draw} & \textbf{Active} \\
\hline
800 & 46\% & 44\% & 10\% & 90\% \\
8,000 & 27\% & 27\% & 46\% & 54\% \\
20,000 & 26\% & 26\% & \textbf{47\%} & 52\% \\
\hline
\end{tabular}
\end{center}
\end{table}

\textbf{Result}: Similar convergence pattern---reward scaling alone does not address the core problem.

\subsubsection{Experiment 3: Strict Draw Penalty}

Modified reward to penalize draws as severely as losses (draw = $-100$).

\begin{table}[h]
\caption{Experiment 3: Strict Draw Penalty (20,000 games)}
\label{tab:exp3}
\begin{center}
\begin{tabular}{|r|c|c|c|c|}
\hline
\textbf{Games} & \textbf{A Win} & \textbf{B Win} & \textbf{Draw} & \textbf{Active} \\
\hline
800 & 45\% & 44\% & 11\% & 89\% \\
8,000 & 25\% & 26\% & 49\% & 51\% \\
20,000 & 32\% & 32\% & \textbf{36\%} & \textbf{64\%} \\
\hline
\end{tabular}
\end{center}
\end{table}

\textbf{Result}: Strict draw penalty successfully reduced draw rate to 36\%, demonstrating that \textbf{reward structure matters more than reward magnitude}.

\subsubsection{Training Curves}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\columnwidth]{figures/sac_experiments_comparison.pdf}
\caption{SAC training comparison. Top-left: Draw rate over training. Top-right: Win rate. All experiments show initial active play followed by convergence toward conservative equilibria, with Experiment 3 (strict draw penalty) achieving lowest final draw rate.}
\label{fig:sac_curves}
\end{figure}

\subsection{Root Cause Analysis}

\textbf{Why does the ``lazy'' equilibrium emerge?} In billiards, attempting to pocket a ball carries risk: missing might leave an easy shot for the opponent, and prematurely pocketing the 8-ball ends the game in a loss. The agent discovers that \textit{not acting decisively} is a stable strategy in symmetric self-play.

\textbf{Fundamental limitations}:
\begin{enumerate}
    \item \textbf{Sparse, delayed rewards}: Meaningful feedback only occurs after complete physics simulation. No gradient signal for ``almost good'' shots.
    
    \item \textbf{High precision requirements}: A 1° error in $\phi$ can mean the difference between pocketing and missing. The reward landscape is discontinuous.
    
    \item \textbf{Exploration inefficiency}: Random Gaussian exploration in 5D space almost never produces successful pockets.
    
    \item \textbf{Credit assignment}: With 20+ shots per game, attributing terminal rewards to specific actions is extremely difficult.
    
    \item \textbf{Self-play dynamics}: Symmetric training finds Nash equilibria that may be suboptimal against fixed opponents.
\end{enumerate}

\textbf{Lesson learned}: For high-precision continuous control with binary outcomes, end-to-end RL requires prohibitive sample complexity. When a fast simulator is available, search-based methods are far more practical.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{IMITATION LEARNING}

Given RL's failure, I pivoted to imitation learning: directly learning from the strong MCTS agent developed by my teammate.

\subsection{Expert Data Collection}

\subsubsection{Data Collection Pipeline}

I implemented a parallel data collection system using ProcessPoolExecutor:
\begin{itemize}
    \item Expert: long\_mcts agent (400 simulations, 64 candidates)
    \item Opponent: BasicAgent (to generate diverse game states)
    \item Collection rate: $\sim$100 games/hour with 100 workers
    \item Total dataset: \textbf{1,787,130 state-action pairs}
\end{itemize}

\subsubsection{State Features (80 dimensions)}

I designed a comprehensive state representation:

\begin{itemize}
    \item \textbf{Cue ball} (3 dims): $(x, y, pocketed)$
    \item \textbf{15 object balls} (45 dims): $(x_i, y_i, pocketed_i)$ for $i \in \{1..15\}$
    \item \textbf{Target mask} (15 dims): binary indicators for valid target balls
    \item \textbf{Pocket positions} (12 dims): $(x_j, y_j)$ for 6 pockets
    \item \textbf{Game statistics} (5 dims): remaining own/opponent balls, targeting 8-ball flag, 8-ball pocketed, target count
\end{itemize}

Pocketed balls are encoded as $(0, 0, 1)$ to distinguish from balls at origin.

\subsubsection{Action Labels (6 dimensions)}

\begin{itemize}
    \item $V_0 \in [0.5, 8.0]$: normalized to $[0, 1]$
    \item $\phi \in [0°, 360°]$: encoded as $(\sin\phi, \cos\phi)$ to handle wraparound
    \item $\theta \in [0°, 90°]$: normalized to $[0, 1]$
    \item $a, b \in [-0.5, 0.5]$: spin parameters, kept as-is
\end{itemize}

\subsection{Network Architectures}

I experimented with five architectures of increasing sophistication:

\subsubsection{Architecture 1: MLP Networks}

Simple feedforward networks with varying capacities:
\begin{itemize}
    \item \textbf{Small}: 8M parameters, 4 hidden layers × 512 units
    \item \textbf{Large}: 26M parameters, 6 hidden layers × 1024 units
\end{itemize}

Loss function: weighted MSE
\begin{equation}
\mathcal{L} = \mathcal{L}_{V_0} + 2\mathcal{L}_{\phi} + \mathcal{L}_{\theta} + 0.5\mathcal{L}_{spin}
\end{equation}
where $\mathcal{L}_{\phi}$ includes angle regularization:
\begin{equation}
\mathcal{L}_{\phi} = MSE(\sin\phi) + MSE(\cos\phi) + 0.1\|\sin^2\phi + \cos^2\phi - 1\|^2
\end{equation}

\subsubsection{Architecture 2: Transformer-based}

Treating balls as a set, I used:
\begin{itemize}
    \item Ball embeddings (position + type encoding)
    \item Multi-head self-attention (8 heads)
    \item Global pooling → action prediction heads
\end{itemize}

\subsubsection{Architecture 3: Discrete $\phi$ Classification}

Recognizing that the $\phi$ angle is the most critical and error-prone parameter, I converted it from regression to classification:
\begin{itemize}
    \item 72 bins (5° resolution)
    \item CrossEntropy loss for $\phi$, MSE for others
    \item Added LayerNorm and logit clamping for numerical stability
\end{itemize}

\subsubsection{Architecture 4: Augmented Training}

Applied geometric symmetry transformations to 4× the training data:
\begin{itemize}
    \item Original
    \item X-mirror: $y \rightarrow 1-y$, $\phi \rightarrow -\phi$
    \item Y-mirror: $x \rightarrow 1-x$, $\phi \rightarrow 180°-\phi$
    \item 180° rotation: $x \rightarrow 1-x$, $y \rightarrow 1-y$, $\phi \rightarrow \phi+180°$
\end{itemize}

\subsubsection{Architecture 5: Equivariant Network}

To exploit the permutation symmetry of object balls, I designed a Set Attention architecture:
\begin{itemize}
    \item \textbf{BallEncoder}: shared MLP for each ball
    \item \textbf{SetAttentionBlocks}: self-attention over ball set (8 layers)
    \item \textbf{CrossAttention}: cue ball queries all object balls
    \item \textbf{Target-weighted pooling}: emphasize balls in target mask
    \item Separate heads for $V_0$, $\phi$ (72 bins), $\theta$, spin
\end{itemize}

Key insight: the network should be invariant to ball permutation but should leverage target mask information to focus on relevant balls.

\subsection{Training Results}

\begin{table}[h]
\caption{Imitation Learning Training Results}
\label{tab:il_results}
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Params} & \textbf{Val Loss} & \textbf{Best Epoch} & \textbf{$\phi$ Acc} \\
\hline
Large MLP & 26M & 0.547 & 40 & - \\
Small MLP & 8M & 0.663 & 47 & - \\
Aug Small & 8M & 0.636 & 106 & - \\
Discrete 72 & 26M & 5.55 & 1 & - \\
Equivariant & 26M & 2.97 & 25 & 16\% \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{The Multi-Modal Action Problem}

The fundamental challenge became clear through analysis: \textbf{for any given state, multiple distinct actions can be equally optimal}.

Consider a state where either Ball 3 or Ball 7 could be pocketed. The MCTS expert might choose Ball 3 in some games and Ball 7 in others, depending on subtle factors or randomness in its search. These correspond to completely different $\phi$ angles (e.g., 45° vs 220°).

When training with MSE loss, the network learns to \textit{average} these actions:
\begin{equation}
\hat{\phi} = \frac{\phi_{Ball3} + \phi_{Ball7}}{2} \approx 132.5°
\end{equation}

This averaged action pockets \textit{neither} ball. The network achieves low training loss by predicting the mean of a multi-modal distribution, but this mean is not a valid action.

\textbf{Attempted solutions}:
\begin{enumerate}
    \item \textbf{Discrete $\phi$}: Converting to classification helps but 72 bins still show only 16\% accuracy
    \item \textbf{Mixture Density Networks}: Would model multiple modes but adds complexity
    \item \textbf{Target conditioning}: If we knew which ball to target, the problem becomes unimodal
\end{enumerate}

\subsection{Overfitting Analysis}

All models showed classic overfitting patterns:
\begin{itemize}
    \item Training loss continued decreasing
    \item Validation loss plateaued or increased
    \item Gap widened over epochs
\end{itemize}

This suggests that 1.78M samples, while large, do not adequately cover the state space for generalization. The billiards state space is effectively infinite, and expert behavior varies based on factors not captured in our 80-dimensional encoding.

\subsection{Lessons Learned}

\begin{enumerate}
    \item \textbf{Multi-modal distributions defeat MSE}: Behavior cloning struggles when experts exhibit varied behavior for similar states.
    
    \item \textbf{Angle accuracy is critical}: With 72 bins (5° each), 16\% accuracy means expected error of $\sim$15 bins = 75°, far too large for precise pocketing.
    
    \item \textbf{Distribution shift}: Even with 1.78M samples, the learned policy visits states not seen during training, leading to compounding errors.
    
    \item \textbf{The right inductive bias matters}: Equivariant networks should help with symmetry, but the core multi-modal problem remains.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{COMPARATIVE ANALYSIS}

\begin{table}[h]
\caption{Agent Performance Comparison}
\label{tab:comparison}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Agent} & \textbf{vs Basic} & \textbf{vs Pro} \\
\hline
Physics-based & 60\% & ~40\%* \\
SAC (RL) & Failed & - \\
Imitation Learning & Not tested & - \\
\hline
BasicAgentPro & 85.4\% & - \\
future (MCTS) & 57.3\% & 33.1\% \\
\textbf{long\_mcts} & \textbf{94.4\%} & \textbf{80.6\%} \\
\hline
\end{tabular}
\end{center}
\footnotesize{*Estimated}
\end{table}

The results demonstrate that MCTS-based agents significantly outperform all learning approaches. Key advantages:

\begin{itemize}
    \item \textbf{Look-ahead}: Evaluates consequences of actions through simulation, avoiding the 20\% ``self-defeat'' rate observed in greedy baselines (Section II-B)
    \item \textbf{No training required}: Works directly from physics engine without sample collection
    \item \textbf{Handles multi-modality}: Naturally explores multiple shot options, bypassing the averaging problem in imitation learning
    \item \textbf{Robustness}: Can test actions with noise perturbations before execution
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CONCLUSION}

This project systematically explored learning-based approaches for billiards AI:

\begin{enumerate}
    \item \textbf{Reinforcement Learning}: Developed complete SAC framework with curriculum learning, but failed due to sparse rewards and exploration challenges in high-precision continuous control.
    
    \item \textbf{Imitation Learning}: Collected 1.78M expert samples, experimented with 5 architectures, but fundamental multi-modal action distribution problem limits accuracy.
    
    \item \textbf{Key Insight}: When a fast, accurate simulator is available, search-based methods (MCTS) outperform learning methods. The simulator enables direct evaluation of action consequences, bypassing the credit assignment and multi-modality challenges that plague learning.
\end{enumerate}

The negative results are themselves valuable: they demonstrate the boundaries of current learning methods and highlight the importance of matching algorithm choice to problem structure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{CONTRIBUTIONS}

\textbf{My contributions}:
\begin{itemize}
    \item Physics-based agent baseline (60\% win rate vs BasicAgent)
    \item Opponent robustness analysis revealing 20\% self-defeat rate in baselines
    \item Complete SAC reinforcement learning framework with parallel training
    \item Three controlled RL experiments analyzing reward structure effects
    \item Imitation learning pipeline: data collection (1.78M samples), 5 network architectures
    \item Parallel evaluation framework for systematic agent comparison
    \item Comprehensive analysis of learning method failure modes
\end{itemize}

\textbf{Teammate's contributions}:
\begin{itemize}
    \item MCTS agent development and optimization
    \item Robustness improvements and future reward design
    \item Final long\_mcts solution (80.6\% vs BasicAgentPro)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thebibliography}{99}

\bibitem{haarnoja2018soft} T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, ``Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,'' in \textit{Proc. ICML}, 2018.

\bibitem{ross2011reduction} S. Ross, G. Gordon, and D. Bagnell, ``A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,'' in \textit{Proc. AISTATS}, 2011.

\bibitem{silver2016mastering} D. Silver et al., ``Mastering the Game of Go with Deep Neural Networks and Tree Search,'' \textit{Nature}, vol. 529, pp. 484-489, 2016.

\bibitem{zaheer2017deep} M. Zaheer et al., ``Deep Sets,'' in \textit{Proc. NeurIPS}, 2017.

\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

\section{Hyperparameters}
\label{app:hyperparams}

\begin{table}[h]
\caption{SAC Training Hyperparameters}
\label{tab:sac_hyperparams}
\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
\multicolumn{2}{|c|}{\textit{Network Architecture}} \\
\hline
Actor hidden layers & [256, 256, 128] \\
Critic hidden layers & [256, 256, 128] \\
Activation & ReLU \\
\hline
\multicolumn{2}{|c|}{\textit{SAC Algorithm}} \\
\hline
Learning rate (actor/critic/$\alpha$) & $3 \times 10^{-4}$ \\
Discount factor $\gamma$ & 0.99 \\
Soft update rate $\tau$ & 0.005 \\
Initial temperature $\alpha_0$ & 1.0 \\
Target entropy & $-\dim(\mathcal{A})$ \\
\hline
\multicolumn{2}{|c|}{\textit{Replay Buffer}} \\
\hline
Buffer size & 100,000--200,000 \\
Batch size & 512 \\
Min buffer before training & 2,000--5,000 \\
Priority exponent $\alpha$ & 0.6 \\
\hline
\multicolumn{2}{|c|}{\textit{Training}} \\
\hline
Parallel workers & 32 \\
Games per batch & 32 \\
Updates per batch & 4 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[h]
\caption{Reward Function Parameters}
\label{tab:reward_params}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Component} & \textbf{Compact} & \textbf{Large} \\
\hline
Win reward & +100 & +1000 \\
Loss reward & $-100$ & $-1000$ \\
Draw penalty (default) & progress-based & progress-based \\
Draw penalty (strict) & $-100$ & $-1000$ \\
\hline
Pocket own ball & $+\frac{n}{N} \times 100$ & $+\frac{n}{N} \times 100$ \\
Pocket opponent ball & $-50$ & $-50$ \\
\hline
Keep turn bonus & +10 & +10 \\
Lose turn penalty & $-5$ & $-5$ \\
\hline
Scratch foul & $-20$ & $-20$ \\
Wrong ball first & $-15$ & $-15$ \\
No rail contact & $-10$ & $-10$ \\
Complete miss & $-25$ & $-25$ \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[h]
\caption{Imitation Learning Hyperparameters}
\label{tab:il_hyperparams}
\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
\multicolumn{2}{|c|}{\textit{Data Collection}} \\
\hline
Expert agent & long\_mcts \\
MCTS simulations & 400 \\
MCTS candidates & 64 \\
Total samples & 1,787,130 \\
Train/Val split & 80\%/20\% \\
\hline
\multicolumn{2}{|c|}{\textit{Large MLP}} \\
\hline
Hidden layers & [1024] $\times$ 6 \\
Parameters & 26M \\
\hline
\multicolumn{2}{|c|}{\textit{Equivariant Network}} \\
\hline
Ball embedding dim & 64 \\
Attention heads & 8 \\
Set attention layers & 8 \\
\hline
\multicolumn{2}{|c|}{\textit{Training}} \\
\hline
Optimizer & AdamW \\
Learning rate & $10^{-4}$ \\
Weight decay & $10^{-4}$ \\
Batch size & 512 \\
Epochs & 100--200 \\
LR scheduler & ReduceOnPlateau \\
\hline
\end{tabular}
\end{center}
\end{table}

\section{State and Action Space Details}
\label{app:spaces}

\subsection{SAC State Encoding (53 dimensions)}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|c|l|}
\hline
\textbf{Component} & \textbf{Dims} & \textbf{Description} \\
\hline
Cue ball & 3 & $(x, y, pocketed)$ \\
Own balls & 21 & 7 balls $\times$ 3, sorted by dist \\
Opponent balls & 21 & 7 balls $\times$ 3, sorted by dist \\
8-ball & 3 & $(x, y, pocketed)$ \\
Game info & 5 & counts, phase, turn \\
\hline
\textbf{Total} & \textbf{53} & \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Action Space}

\textbf{Full action space} (5 dimensions):
\begin{itemize}
    \item $V_0 \in [0.5, 8.0]$ m/s: cue velocity
    \item $\phi \in [0°, 360°)$: horizontal angle
    \item $\theta \in [0°, 90°]$: vertical angle (elevation)
    \item $a \in [-0.5, 0.5]$: horizontal spin offset
    \item $b \in [-0.5, 0.5]$: vertical spin offset
\end{itemize}

\textbf{Simplified action space} (2 dimensions, used in experiments):
\begin{itemize}
    \item $V_0 \in [1.0, 6.0]$ m/s
    \item $\phi \in [0°, 360°)$
\end{itemize}
Fixed: $\theta = 0°$, $a = b = 0$ (no spin).

\end{document}
