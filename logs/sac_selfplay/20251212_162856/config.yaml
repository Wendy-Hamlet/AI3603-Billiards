action:
  action_dim: 5
  ranges:
    V0:
    - 0.5
    - 8.0
    a:
    - -0.4
    - 0.4
    b:
    - -0.4
    - 0.4
    phi:
    - 0.0
    - 360.0
    theta:
    - 0.0
    - 45.0
curriculum:
  enabled: true
  stages:
  - description: "\u7B80\u5355\u73AF\u5883\uFF1A\u5DF1\u65B91\u7403+\u5BF9\u65B91\u7403\
      +\u9ED18"
    enemy_balls: 1
    episodes: 50000
    name: stage_1
    own_balls: 1
  - description: "\u4E2D\u7B49\u73AF\u5883\uFF1A\u5DF1\u65B93\u7403+\u5BF9\u65B93\u7403\
      +\u9ED18"
    enemy_balls: 3
    episodes: 100000
    name: stage_2
    own_balls: 3
  - description: "\u8F83\u96BE\u73AF\u5883\uFF1A\u5DF1\u65B95\u7403+\u5BF9\u65B95\u7403\
      +\u9ED18"
    enemy_balls: 5
    episodes: 150000
    name: stage_3
    own_balls: 5
  - description: "\u5B8C\u6574\u73AF\u5883\uFF1A\u5DF1\u65B97\u7403+\u5BF9\u65B97\u7403\
      +\u9ED18"
    enemy_balls: 7
    episodes: 200000
    name: stage_4
    own_balls: 7
logging:
  log_dir: logs/sac_selfplay
  save_dir: checkpoints/sac_selfplay
  tensorboard: true
  wandb: false
network:
  actor:
    activation: relu
    hidden_dims:
    - 512
    - 512
    - 256
  critic:
    activation: relu
    hidden_dims:
    - 512
    - 512
    - 256
reward:
  w_defense: 0.0
  w_foul: 1.0
  w_pocket: 1.0
  w_position: 0.3
  w_terminal: 1.0
state:
  state_dim: 64
  symmetric_encoding: true
training:
  batch_size: 2048
  buffer_size: 500000
  defense_reward_enabled: false
  eval_freq: 1000
  games_per_batch: 64
  gamma: 0.99
  initial_alpha: 0.2
  learning_batch_size: 128
  log_freq: 100
  lr_actor: 0.0003
  lr_alpha: 0.0003
  lr_critic: 0.0003
  min_buffer_size: 10000
  num_workers: 1
  save_freq: 5000
  seed: 42
  self_play: true
  target_entropy: null
  tau: 0.005
  total_episodes: 500000
  updates_per_batch: 8
