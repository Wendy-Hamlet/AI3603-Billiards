"""
Train SAC - SACè®­ç»ƒä¸»è„šæœ¬
å®ç°æ¸è¿›å¼è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæµç¨‹
"""

import os
import sys
import time
import numpy as np
import torch
from datetime import datetime

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from config import (
    TRAINING_STAGES, EVAL_CONFIG, CHECKPOINT_CONFIG, LOG_CONFIG,
    DEVICE, SAC_CONFIG
)
from state_encoder import StateEncoder
from reward_shaper import RewardShaper, get_ball_ids_by_type, count_remaining_balls
from sac_agent import SACAgent, SACAgentWrapper
from replay_buffer import ReplayBuffer, EpisodeTracker
from opponent_pool import OpponentPool
from poolenv import PoolEnv


class SACTrainer:
    """SAC è®­ç»ƒå™¨"""
    
    def __init__(self, resume_from=None):
        """
        Args:
            resume_from: str, checkpoint è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºæ¢å¤è®­ç»ƒï¼‰
        """
        print("=" * 60)
        print("åˆå§‹åŒ– SAC è®­ç»ƒå™¨")
        print("=" * 60)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.state_encoder = StateEncoder()
        self.reward_shaper = RewardShaper()
        self.sac_agent = SACAgent()
        self.sac_wrapper = SACAgentWrapper(self.sac_agent, self.state_encoder)
        self.replay_buffer = ReplayBuffer(capacity=SAC_CONFIG['replay_buffer_size'])
        self.opponent_pool = OpponentPool()
        self.env = PoolEnv()
        
        # è®­ç»ƒçŠ¶æ€
        self.global_episode = 0
        self.current_stage = 'stage1'
        self.stage_episode = 0
        
        # ç»Ÿè®¡
        self.episode_rewards = []
        self.episode_lengths = []
        self.win_counts = {'basic': 0, 'physics': 0, 'mcts': 0}
        self.game_counts = {'basic': 0, 'physics': 0, 'mcts': 0}
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(CHECKPOINT_CONFIG['save_dir'], exist_ok=True)
        os.makedirs(LOG_CONFIG['log_dir'], exist_ok=True)
        
        # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if resume_from:
            self._load_checkpoint(resume_from)
        
        print(f"âœ… åˆå§‹åŒ–å®Œæˆ")
        print(f"   Device: {DEVICE}")
        print(f"   Replay Buffer Capacity: {SAC_CONFIG['replay_buffer_size']}")
        print(f"   Training Stages: {len(TRAINING_STAGES)}")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("\n" + "=" * 60)
        print("å¼€å§‹è®­ç»ƒ")
        print("=" * 60)
        
        # é¢„çƒ­é˜¶æ®µï¼šéšæœºç­–ç•¥å¡«å…… buffer
        if len(self.replay_buffer) < SAC_CONFIG['warmup_steps']:
            self._warmup()
        
        # æ¸è¿›å¼è®­ç»ƒ
        for stage_name, stage_config in TRAINING_STAGES.items():
            if self._should_skip_stage(stage_name):
                continue
            
            self.current_stage = stage_name
            self.stage_episode = 0
            
            print("\n" + "=" * 60)
            print(f"é˜¶æ®µ: {stage_config['name']}")
            print(f"ç›®æ ‡ Episodes: {stage_config['episodes']}")
            print(f"å¯¹æ‰‹åˆ†å¸ƒ: {stage_config['opponents']}")
            print("=" * 60)
            
            # é˜¶æ®µè®­ç»ƒå¾ªç¯
            while self.stage_episode < stage_config['episodes']:
                # è®­ç»ƒä¸€ä¸ª episode
                episode_info = self._train_episode(stage_config)
                
                self.global_episode += 1
                self.stage_episode += 1
                
                # è®°å½•ç»Ÿè®¡
                self._log_episode(episode_info)
                
                # å®šæœŸè¯„ä¼°
                if self.global_episode % EVAL_CONFIG['eval_frequency'] == 0:
                    eval_results = self._evaluate()
                    self._log_evaluation(eval_results)
                    
                    # æ£€æŸ¥æ˜¯å¦æå‰å®Œæˆé˜¶æ®µ
                    if self._check_stage_completion(stage_config, eval_results):
                        print(f"\nâœ… é˜¶æ®µ {stage_name} æå‰å®Œæˆï¼")
                        break
                
                # ä¿å­˜ checkpoint
                if self.global_episode % EVAL_CONFIG['checkpoint_frequency'] == 0:
                    self._save_checkpoint()
                    
                    # æ·»åŠ åˆ° self-play æ± 
                    if self.global_episode >= 5000:  # è®­ç»ƒè¶³å¤Ÿä¹…åæ‰åŠ å…¥
                        self.opponent_pool.add_checkpoint(
                            self.sac_wrapper,
                            self.global_episode,
                            {'episode': self.global_episode}
                        )
        
        print("\n" + "=" * 60)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        
        # æœ€ç»ˆè¯„ä¼°
        final_eval = self._evaluate()
        self._log_evaluation(final_eval, final=True)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_checkpoint(is_final=True)
    
    def _warmup(self):
        """é¢„çƒ­é˜¶æ®µï¼šä½¿ç”¨éšæœºç­–ç•¥å¡«å…… replay buffer"""
        print("\n" + "-" * 60)
        print(f"é¢„çƒ­é˜¶æ®µï¼šéšæœºç­–ç•¥å¡«å…… buffer åˆ° {SAC_CONFIG['warmup_steps']} ä¸ª transitions")
        print("-" * 60)
        
        while len(self.replay_buffer) < SAC_CONFIG['warmup_steps']:
            # éšæœºå¯¹æ‰‹
            opponent = self.opponent_pool.get_opponent('basic')
            
            # ç©ä¸€å±€æ¸¸æˆ
            self.env.reset(target_ball='solid')
            done = False
            
            while not done:
                current_player = self.env.get_curr_player()
                
                if current_player == 'A':  # SAC agent
                    # ç¼–ç çŠ¶æ€
                    state = self.state_encoder.encode_from_env(self.env, 'A')
                    
                    # éšæœºåŠ¨ä½œ
                    action = np.random.uniform(-1, 1, SAC_CONFIG['action_dim'])
                    
                    # æ‰§è¡ŒåŠ¨ä½œ
                    from config import denormalize_action
                    action_dict = denormalize_action(action)
                    shot_result = self.env.take_shot(**action_dict)
                    
                    # è®¡ç®—å¥–åŠ±
                    my_balls_before = count_remaining_balls(
                        self.env.balls,
                        get_ball_ids_by_type(self.env.player_targets['A'][0])
                    )
                    enemy_balls_before = count_remaining_balls(
                        self.env.balls,
                        get_ball_ids_by_type(self.env.player_targets['B'][0])
                    )
                    
                    reward = self.reward_shaper.calculate_immediate_reward(
                        shot_result, my_balls_before, enemy_balls_before
                    )
                    
                    next_state = self.state_encoder.encode_from_env(self.env, 'A')
                    done = self.env.get_done()[0]
                    
                    # å­˜å‚¨
                    self.replay_buffer.push(state, action, reward, next_state, done)
                
                else:  # å¯¹æ‰‹
                    balls, my_type, table = self.env.get_observation()
                    action_dict = opponent.decision(balls, my_type, table)
                    self.env.take_shot(**action_dict)
                    done = self.env.get_done()[0]
            
            if len(self.replay_buffer) % 1000 == 0:
                print(f"  Buffer size: {len(self.replay_buffer)}/{SAC_CONFIG['warmup_steps']}")
        
        print(f"âœ… é¢„çƒ­å®Œæˆï¼Œbuffer size: {len(self.replay_buffer)}")
    
    def _train_episode(self, stage_config):
        """è®­ç»ƒä¸€ä¸ª episode"""
        # é€‰æ‹©å¯¹æ‰‹
        opponent = self.opponent_pool.sample_opponent(stage_config)
        opponent_type = self._identify_opponent_type(opponent)
        
        # é‡ç½®ç¯å¢ƒ
        target_ball = 'solid' if self.global_episode % 2 == 0 else 'stripe'
        self.env.reset(target_ball=target_ball)
        
        # Episode è¿½è¸ªå™¨
        tracker = EpisodeTracker()
        
        episode_reward = 0.0
        episode_length = 0
        done = False
        
        while not done:
            current_player = self.env.get_curr_player()
            
            if current_player == 'A':  # SAC agent å›åˆ
                # ç¼–ç çŠ¶æ€
                state = self.state_encoder.encode_from_env(self.env, 'A')
                
                # é€‰æ‹©åŠ¨ä½œ
                action = self.sac_agent.select_action(state, deterministic=False)
                
                # è®°å½•å‡»çƒå‰çš„çŠ¶æ€
                my_type = self.env.player_targets['A'][0]
                my_ball_ids = get_ball_ids_by_type(my_type)
                enemy_type = 'stripe' if my_type == 'solid' else 'solid'
                enemy_ball_ids = get_ball_ids_by_type(enemy_type)
                
                my_balls_before = count_remaining_balls(self.env.balls, my_ball_ids)
                enemy_balls_before = count_remaining_balls(self.env.balls, enemy_ball_ids)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                from config import denormalize_action
                action_dict = denormalize_action(action)
                shot_result = self.env.take_shot(**action_dict)
                
                # è®¡ç®—å¥–åŠ±
                game_done, info = self.env.get_done()
                i_won = None
                if game_done:
                    i_won = (info.get('winner') == 'A')
                
                reward = self.reward_shaper.calculate_immediate_reward(
                    shot_result, my_balls_before, enemy_balls_before,
                    game_done, i_won
                )
                
                next_state = self.state_encoder.encode_from_env(self.env, 'A')
                
                # å­˜å‚¨ transition
                buffer_idx = self.replay_buffer.push(
                    state, action, reward, next_state, game_done,
                    meta_info={'is_sac_turn': True}
                )
                tracker.add_transition(buffer_idx, is_sac_turn=True)
                
                episode_reward += reward
                episode_length += 1
                done = game_done
                
            else:  # å¯¹æ‰‹å›åˆ
                balls, my_type, table = self.env.get_observation()
                action_dict = opponent.decision(balls, my_type, table)
                shot_result = self.env.take_shot(**action_dict)
                
                # æ£€æŸ¥å¯¹æ‰‹æ˜¯å¦å¤±è¯¯ï¼Œè¿½æº¯é˜²å®ˆå¥–åŠ±
                if shot_result.get('WHITE_BALL_INTO_POCKET') or \
                   shot_result.get('FOUL_FIRST_HIT') or \
                   shot_result.get('NO_POCKET_NO_RAIL') or \
                   shot_result.get('NO_HIT') or \
                   not shot_result.get('ME_INTO_POCKET'):
                    
                    last_sac_idx = tracker.get_last_sac_turn_idx()
                    if last_sac_idx is not None:
                        defense_reward = self.reward_shaper.calculate_defense_reward(shot_result)
                        self.replay_buffer.add_defense_reward(last_sac_idx, defense_reward)
                        episode_reward += defense_reward
                
                tracker.add_transition(-1, is_sac_turn=False)
                done = self.env.get_done()[0]
            
            # è®­ç»ƒæ›´æ–°
            if len(self.replay_buffer) >= SAC_CONFIG['warmup_steps']:
                for _ in range(SAC_CONFIG['gradient_steps']):
                    self.sac_agent.update(self.replay_buffer, SAC_CONFIG['batch_size'])
        
        # è®°å½•èƒœè´Ÿ
        game_done, info = self.env.get_done()
        if game_done:
            winner = info.get('winner')
            self.game_counts[opponent_type] += 1
            if winner == 'A':
                self.win_counts[opponent_type] += 1
        
        return {
            'reward': episode_reward,
            'length': episode_length,
            'opponent_type': opponent_type,
            'won': (winner == 'A') if game_done else False
        }
    
    def _identify_opponent_type(self, opponent):
        """è¯†åˆ«å¯¹æ‰‹ç±»å‹"""
        from agent import BasicAgent, NewAgent
        if isinstance(opponent, BasicAgent):
            return 'basic'
        elif isinstance(opponent, NewAgent):
            return 'physics'
        else:
            return 'self'
    
    def _evaluate(self):
        """è¯„ä¼°å½“å‰æ¨¡å‹"""
        print("\n" + "-" * 60)
        print(f"è¯„ä¼°ä¸­... (Episode {self.global_episode})")
        
        # åˆ‡æ¢åˆ°ç¡®å®šæ€§ç­–ç•¥
        self.sac_wrapper.set_deterministic(True)
        
        results = {}
        for opponent_type in ['basic', 'physics']:
            wins = 0
            games = EVAL_CONFIG['eval_games']
            
            for i in range(games):
                opponent = self.opponent_pool.get_opponent(opponent_type)
                target_ball = 'solid' if i % 2 == 0 else 'stripe'
                self.env.reset(target_ball=target_ball)
                
                done = False
                while not done:
                    current_player = self.env.get_curr_player()
                    
                    if current_player == 'A':
                        balls, my_type, table = self.env.get_observation()
                        action_dict = self.sac_wrapper.decision(balls, my_type, table)
                        self.env.take_shot(**action_dict)
                    else:
                        balls, my_type, table = self.env.get_observation()
                        action_dict = opponent.decision(balls, my_type, table)
                        self.env.take_shot(**action_dict)
                    
                    done = self.env.get_done()[0]
                
                game_done, info = self.env.get_done()
                if info.get('winner') == 'A':
                    wins += 1
            
            winrate = wins / games
            results[f'{opponent_type}_winrate'] = winrate
            print(f"  vs {opponent_type}: {wins}/{games} = {winrate:.1%}")
        
        # æ¢å¤éšæœºç­–ç•¥
        self.sac_wrapper.set_deterministic(False)
        
        return results
    
    def _check_stage_completion(self, stage_config, eval_results):
        """æ£€æŸ¥é˜¶æ®µæ˜¯å¦æå‰å®Œæˆ"""
        target_metrics = stage_config.get('target_metrics', {})
        
        for metric_name, target_value in target_metrics.items():
            if eval_results.get(metric_name, 0) >= target_value:
                return True
        
        return False
    
    def _should_skip_stage(self, stage_name):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡æŸä¸ªé˜¶æ®µï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰"""
        # ç®€å•å®ç°ï¼šæŒ‰é¡ºåºè®­ç»ƒ
        stages = list(TRAINING_STAGES.keys())
        current_idx = stages.index(self.current_stage)
        target_idx = stages.index(stage_name)
        return target_idx < current_idx
    
    def _log_episode(self, episode_info):
        """è®°å½• episode ä¿¡æ¯"""
        if self.global_episode % EVAL_CONFIG['log_frequency'] == 0:
            stats = self.sac_agent.get_statistics()
            print(f"\nEpisode {self.global_episode} (Stage: {self.current_stage}, {self.stage_episode})")
            print(f"  Reward: {episode_info['reward']:.2f}")
            print(f"  Length: {episode_info['length']}")
            print(f"  Opponent: {episode_info['opponent_type']}")
            print(f"  Alpha: {stats.get('alpha_mean', 0):.4f}")
            print(f"  Buffer: {len(self.replay_buffer)}")
    
    def _log_evaluation(self, eval_results, final=False):
        """è®°å½•è¯„ä¼°ç»“æœ"""
        prefix = "æœ€ç»ˆè¯„ä¼°" if final else "è¯„ä¼°ç»“æœ"
        print(f"\n{prefix} (Episode {self.global_episode}):")
        for metric, value in eval_results.items():
            print(f"  {metric}: {value:.1%}")
    
    def _save_checkpoint(self, is_final=False):
        """ä¿å­˜ checkpoint"""
        filename = f"final_model.pth" if is_final else f"checkpoint_ep{self.global_episode}.pth"
        filepath = os.path.join(CHECKPOINT_CONFIG['save_dir'], filename)
        
        self.sac_agent.save(filepath)
        print(f"ğŸ’¾ Checkpoint å·²ä¿å­˜: {filepath}")
    
    def _load_checkpoint(self, filepath):
        """åŠ è½½ checkpoint"""
        self.sac_agent.load(filepath)
        print(f"ğŸ“‚ Checkpoint å·²åŠ è½½: {filepath}")


# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è®­ç»ƒ SAC Agent')
    parser.add_argument('--resume', type=str, default=None, help='æ¢å¤è®­ç»ƒçš„ checkpoint è·¯å¾„')
    parser.add_argument('--test', action='store_true', help='ä½¿ç”¨æµ‹è¯•é…ç½®ï¼ˆå°‘é‡ episodesï¼‰')
    args = parser.parse_args()
    
    # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ï¼Œä¿®æ”¹é…ç½®
    if args.test:
        print("âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨å‡å°‘çš„ episodes")
        from config import get_quick_test_config
        TRAINING_STAGES.update(get_quick_test_config())
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SACTrainer(resume_from=args.resume)
    
    # å¼€å§‹è®­ç»ƒ
    try:
        trainer.train()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        trainer._save_checkpoint()
        print("æ¨¡å‹å·²ä¿å­˜")
    except Exception as e:
        print(f"\n\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        trainer._save_checkpoint()
        print("æ¨¡å‹å·²ä¿å­˜")


if __name__ == '__main__':
    main()
