"""
Train SAC Parallel - å¹¶è¡Œ SAC è®­ç»ƒä¸»è„šæœ¬
å®ç°æ‰¹é‡å¹¶è¡Œå¯¹å±€ä»¥åŠ é€Ÿè®­ç»ƒ
"""

import os
import sys

# ==================== CUDA ç¯å¢ƒè®¾ç½® ====================
if 'CUDA_VISIBLE_DEVICES' not in os.environ:
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import time
import numpy as np
import torch
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict

# æ£€æŸ¥ CUDA ç¯å¢ƒ
print("="*60)
print("ç¯å¢ƒæ£€æŸ¥")
print("="*60)
print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"GPU æ•°é‡: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
print()

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from config import (
    TRAINING_STAGES, EVAL_CONFIG, CHECKPOINT_CONFIG, LOG_CONFIG,
    DEVICE, SAC_CONFIG, denormalize_action
)
from state_encoder import StateEncoder
from reward_shaper import RewardShaper, get_ball_ids_by_type, count_remaining_balls
from sac_agent import SACAgent, SACAgentWrapper
from replay_buffer import ReplayBuffer, EpisodeTracker
from opponent_pool import OpponentPool
from poolenv import PoolEnv


class ParallelSACTrainer:
    """å¹¶è¡Œ SAC è®­ç»ƒå™¨ - æ‰¹é‡è¿è¡Œå¤šä¸ªç¯å¢ƒå®ä¾‹"""
    
    def __init__(self, resume_from=None):
        """
        Args:
            resume_from: str, checkpoint è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºæ¢å¤è®­ç»ƒï¼‰
        """
        print("=" * 60)
        print("åˆå§‹åŒ–å¹¶è¡Œ SAC è®­ç»ƒå™¨")
        print("=" * 60)
        
        # åˆå§‹åŒ–ç»„ä»¶ï¼ˆå…±äº«ï¼‰
        self.state_encoder = StateEncoder()
        self.reward_shaper = RewardShaper()
        self.sac_agent = SACAgent()
        self.sac_wrapper = SACAgentWrapper(self.sac_agent, self.state_encoder)
        self.replay_buffer = ReplayBuffer(capacity=SAC_CONFIG['replay_buffer_size'])
        self.opponent_pool = OpponentPool()
        
        # å¹¶è¡Œé…ç½®
        self.num_parallel_envs = SAC_CONFIG['num_parallel_envs']
        self.update_frequency = SAC_CONFIG['update_frequency']
        
        # è®­ç»ƒçŠ¶æ€
        self.global_episode = 0
        self.current_stage = 'stage1'
        self.stage_episode = 0
        
        # ç»Ÿè®¡
        self.episode_rewards = []
        self.episode_lengths = []
        self.win_counts = {'basic': 0, 'physics': 0, 'mcts': 0, 'self': 0}
        self.game_counts = {'basic': 0, 'physics': 0, 'mcts': 0, 'self': 0}
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(CHECKPOINT_CONFIG['save_dir'], exist_ok=True)
        os.makedirs(LOG_CONFIG['log_dir'], exist_ok=True)
        
        # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if resume_from:
            self._load_checkpoint(resume_from)
        
        print(f"âœ… åˆå§‹åŒ–å®Œæˆ")
        print(f"   Device: {DEVICE}")
        print(f"   å¹¶è¡Œç¯å¢ƒæ•°: {self.num_parallel_envs}")
        print(f"   æ›´æ–°é¢‘ç‡: æ¯ {self.update_frequency} episodes")
        print(f"   Replay Buffer Capacity: {SAC_CONFIG['replay_buffer_size']}")
        print(f"   Training Stages: {len(TRAINING_STAGES)}")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("\n" + "=" * 60)
        print("å¼€å§‹å¹¶è¡Œè®­ç»ƒ")
        print("=" * 60)
        
        # é¢„çƒ­é˜¶æ®µ
        if len(self.replay_buffer) < SAC_CONFIG['warmup_steps']:
            self._warmup_parallel()
        
        # éå†è®­ç»ƒé˜¶æ®µ
        for stage_name, stage_config in TRAINING_STAGES.items():
            if self.current_stage != stage_name:
                continue  # è·³è¿‡å·²å®Œæˆçš„é˜¶æ®µ
            
            self._train_stage(stage_name, stage_config)
            
            # å®Œæˆå½“å‰é˜¶æ®µï¼Œè¿›å…¥ä¸‹ä¸€é˜¶æ®µ
            self.current_stage = self._get_next_stage(stage_name)
            self.stage_episode = 0
        
        print("\n" + "=" * 60)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
    
    def _warmup_parallel(self):
        """å¹¶è¡Œé¢„çƒ­ - ä½¿ç”¨éšæœºç­–ç•¥å¡«å…… buffer"""
        print("\n" + "-" * 60)
        print(f"é¢„çƒ­é˜¶æ®µï¼šå¹¶è¡Œéšæœºç­–ç•¥å¡«å…… buffer åˆ° {SAC_CONFIG['warmup_steps']} ä¸ª transitions")
        print(f"   ç¯å¢ƒå™ªå£°: ç¦ç”¨")
        print(f"   å¯¹æ‰‹: RandomAgent (å¿«é€Ÿéšæœº)")
        print(f"   å¹¶è¡Œç¯å¢ƒæ•°: {self.num_parallel_envs}")
        print("-" * 60)
        
        while len(self.replay_buffer) < SAC_CONFIG['warmup_steps']:
            # å¹¶è¡Œè¿è¡Œå¤šä¸ªepisode
            batch_results = self._run_parallel_episodes_warmup(self.num_parallel_envs)
            
            # å­˜å‚¨æ‰€æœ‰transitions
            for transitions in batch_results:
                for transition in transitions:
                    self.replay_buffer.push(*transition)
            
            if len(self.replay_buffer) % 1000 == 0 or len(self.replay_buffer) >= SAC_CONFIG['warmup_steps']:
                progress = min(100.0, (len(self.replay_buffer) / SAC_CONFIG['warmup_steps']) * 100)
                print(f"  ğŸ“¦ Buffer: {len(self.replay_buffer):5d}/{SAC_CONFIG['warmup_steps']} [{progress:5.1f}%]")
        
        print(f"âœ… é¢„çƒ­å®Œæˆï¼Œbuffer size: {len(self.replay_buffer)}")
    
    def _run_parallel_episodes_warmup(self, num_episodes):
        """å¹¶è¡Œè¿è¡Œå¤šä¸ªwarmup episodes"""
        with ThreadPoolExecutor(max_workers=self.num_parallel_envs) as executor:
            futures = [
                executor.submit(self._run_single_episode_warmup)
                for _ in range(num_episodes)
            ]
            
            results = []
            for future in as_completed(futures):
                try:
                    transitions = future.result()
                    results.append(transitions)
                except Exception as e:
                    print(f"âŒ Warmup episode error: {e}")
            
            return results
    
    def _run_single_episode_warmup(self):
        """è¿è¡Œå•ä¸ªwarmup episodeå¹¶è¿”å›transitions"""
        # æ¯ä¸ªçº¿ç¨‹åˆ›å»ºè‡ªå·±çš„ç¯å¢ƒå®ä¾‹
        env = PoolEnv(verbose=False)
        env.enable_noise = False
        opponent = self.opponent_pool.get_opponent('random')
        
        transitions = []
        env.reset(target_ball='solid')
        done = False
        
        while not done:
            current_player = env.get_curr_player()
            
            if current_player == 'A':  # SAC agent
                state = self.state_encoder.encode_from_env(env, 'A')
                action = np.random.uniform(-1, 1, SAC_CONFIG['action_dim'])
                
                action_dict = denormalize_action(action)
                shot_result = env.take_shot(action_dict)
                
                my_type = env.player_targets['A'][0]
                enemy_type = 'stripe' if my_type == 'solid' else 'solid'
                my_balls_before = count_remaining_balls(
                    env.balls, get_ball_ids_by_type(my_type)
                )
                enemy_balls_before = count_remaining_balls(
                    env.balls, get_ball_ids_by_type(enemy_type)
                )
                
                reward = self.reward_shaper.calculate_immediate_reward(
                    shot_result, my_balls_before, enemy_balls_before
                )
                
                next_state = self.state_encoder.encode_from_env(env, 'A')
                done = env.get_done()[0]
                
                transitions.append((state, action, reward, next_state, done))
            
            else:  # å¯¹æ‰‹
                balls, my_type, table = env.get_observation()
                action_dict = opponent.decision(balls, my_type, table)
                env.take_shot(action_dict)
                done = env.get_done()[0]
        
        return transitions
    
    def _train_stage(self, stage_name, stage_config):
        """è®­ç»ƒä¸€ä¸ªé˜¶æ®µ"""
        print("\n" + "=" * 80)
        print(f"ğŸ¯ é˜¶æ®µ {stage_name}: {stage_config['name']}")
        print("=" * 80)
        print(f"  ğŸ“ˆ ç›®æ ‡ Episodes: {stage_config['episodes']}")
        print(f"  ğŸ¤– å¯¹æ‰‹åˆ†å¸ƒ: {stage_config['opponents']}")
        print(f"  ğŸ† å®Œæˆæ¡ä»¶: {stage_config['target_metrics']}")
        print("=" * 80)
        print(f"Episode |     Stage Progress |                  Reward |   Steps |          Result")
        print("-" * 80)
        
        stage_start_episode = self.global_episode
        target_episodes = stage_config['episodes']
        
        # æ”¶é›†ä¸€æ‰¹episodesåå†æ›´æ–°
        episode_batch = []
        
        while self.stage_episode < target_episodes:
            # å¹¶è¡Œè¿è¡Œä¸€æ‰¹episodes
            batch_results = self._run_parallel_episodes_train(
                self.update_frequency,
                stage_config
            )
            
            # å¤„ç†ç»“æœå¹¶å­˜å‚¨åˆ°buffer
            for ep_info in batch_results:
                self.global_episode += 1
                self.stage_episode += 1
                
                # æ›´æ–°ç»Ÿè®¡
                self.episode_rewards.append(ep_info['reward'])
                self.episode_lengths.append(ep_info['length'])
                self.game_counts[ep_info['opponent']] += 1
                if ep_info['won']:
                    self.win_counts[ep_info['opponent']] += 1
                
                # å­˜å‚¨transitions
                for transition in ep_info['transitions']:
                    self.replay_buffer.push(*transition)
                
                # æ‰“å°è¿›åº¦
                self._log_episode(ep_info, stage_name, target_episodes)
                
                # è¯¦ç»†ç»Ÿè®¡
                if self.global_episode % LOG_CONFIG['detailed_log_frequency'] == 0:
                    self._print_detailed_stats()
                
                # è¯„ä¼°
                if self.global_episode % EVAL_CONFIG['eval_frequency'] == 0:
                    self._evaluate()
                
                # ä¿å­˜checkpoint
                if self.global_episode % EVAL_CONFIG['checkpoint_frequency'] == 0:
                    self._save_checkpoint()
            
            # æ‰¹é‡æ›´æ–°ç½‘ç»œï¼ˆæ”¶é›†å®Œä¸€æ‰¹episodesåï¼‰
            self._batch_update_network(len(batch_results))
        
        print(f"\nâœ… é˜¶æ®µ {stage_name} å®Œæˆ")
    
    def _run_parallel_episodes_train(self, num_episodes, stage_config):
        """å¹¶è¡Œè¿è¡Œå¤šä¸ªè®­ç»ƒepisodes"""
        with ThreadPoolExecutor(max_workers=self.num_parallel_envs) as executor:
            futures = [
                executor.submit(self._run_single_episode_train, stage_config)
                for _ in range(num_episodes)
            ]
            
            results = []
            for future in as_completed(futures):
                try:
                    ep_info = future.result()
                    results.append(ep_info)
                except Exception as e:
                    print(f"âŒ Training episode error: {e}")
                    import traceback
                    traceback.print_exc()
            
            return results
    
    def _run_single_episode_train(self, stage_config):
        """è¿è¡Œå•ä¸ªè®­ç»ƒepisode"""
        # æ¯ä¸ªçº¿ç¨‹åˆ›å»ºè‡ªå·±çš„ç¯å¢ƒå®ä¾‹
        env = PoolEnv(verbose=False)
        env.enable_noise = False
        
        # é€‰æ‹©å¯¹æ‰‹
        opponent = self.opponent_pool.sample_opponent(stage_config)
        opponent_type = self._identify_opponent_type(opponent)
        
        transitions = []
        target_ball = 'solid' if np.random.rand() < 0.5 else 'stripe'
        env.reset(target_ball=target_ball)
        
        episode_reward = 0.0
        episode_length = 0
        done = False
        
        while not done:
            current_player = env.get_curr_player()
            
            if current_player == 'A':  # SAC agent
                state = self.state_encoder.encode_from_env(env, 'A')
                
                # ä½¿ç”¨å…±äº«çš„SAC agentï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                with torch.no_grad():
                    action = self.sac_agent.select_action(state, deterministic=False)
                
                my_type = env.player_targets['A'][0]
                enemy_type = 'stripe' if my_type == 'solid' else 'solid'
                my_balls_before = count_remaining_balls(
                    env.balls, get_ball_ids_by_type(my_type)
                )
                enemy_balls_before = count_remaining_balls(
                    env.balls, get_ball_ids_by_type(enemy_type)
                )
                
                action_dict = denormalize_action(action)
                shot_result = env.take_shot(action_dict)
                
                reward = self.reward_shaper.calculate_immediate_reward(
                    shot_result, my_balls_before, enemy_balls_before
                )
                
                next_state = self.state_encoder.encode_from_env(env, 'A')
                done = env.get_done()[0]
                
                transitions.append((state, action, reward, next_state, done))
                episode_reward += reward
                episode_length += 1
            
            else:  # å¯¹æ‰‹
                balls, my_type, table = env.get_observation()
                action_dict = opponent.decision(balls, my_type, table)
                env.take_shot(action_dict)
                done = env.get_done()[0]
        
        # æ£€æŸ¥èƒœè´Ÿ
        winner = env.get_winner()
        won = (winner == 'A')
        
        return {
            'transitions': transitions,
            'reward': episode_reward,
            'length': episode_length,
            'opponent': opponent_type,
            'won': won
        }
    
    def _batch_update_network(self, num_episodes):
        """æ‰¹é‡æ›´æ–°ç½‘ç»œ - åœ¨æ”¶é›†å®Œä¸€æ‰¹episodesåæ‰§è¡Œ"""
        if len(self.replay_buffer) < SAC_CONFIG['batch_size']:
            return
        
        # æ ¹æ®episodeæ•°é‡å†³å®šæ›´æ–°æ¬¡æ•°
        # æ¯ä¸ªepisodeå¹³å‡10-20æ­¥ï¼Œæ‰€ä»¥æ›´æ–°æ¬¡æ•° = num_episodes * 15 * gradient_steps
        update_steps = num_episodes * 15 * SAC_CONFIG['gradient_steps']
        
        for _ in range(update_steps):
            batch = self.replay_buffer.sample(SAC_CONFIG['batch_size'])
            self.sac_agent.update(batch)
    
    def _identify_opponent_type(self, opponent):
        """è¯†åˆ«å¯¹æ‰‹ç±»å‹"""
        class_name = opponent.__class__.__name__
        if 'Random' in class_name:
            return 'random'
        elif 'Basic' in class_name:
            return 'basic'
        elif 'Physics' in class_name:
            return 'physics'
        elif 'MCTS' in class_name:
            return 'mcts'
        elif 'SAC' in class_name or 'Wrapper' in class_name:
            return 'self'
        else:
            return 'unknown'
    
    def _log_episode(self, ep_info, stage_name, target_episodes):
        """è®°å½•episodeä¿¡æ¯"""
        if self.global_episode % LOG_CONFIG['log_frequency'] != 0:
            return
        
        progress = (self.stage_episode / target_episodes) * 100
        avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)
        result_symbol = 'âœ“' if ep_info['won'] else 'âœ—'
        
        print(f"Ep {self.global_episode:5d} | Stage {stage_name} [{progress:5.1f}%] | "
              f"Reward: {ep_info['reward']:7.2f} (avg100: {avg_reward:7.2f}) | "
              f"Steps: {ep_info['length']:2d} | {result_symbol} vs {ep_info['opponent']:7s}")
    
    def _print_detailed_stats(self):
        """æ‰“å°è¯¦ç»†ç»Ÿè®¡"""
        print("\n" + "=" * 80)
        print(f"ğŸ“Š è¯¦ç»†ç»Ÿè®¡ (Episode {self.global_episode})")
        print("=" * 80)
        
        recent_rewards = self.episode_rewards[-100:]
        print(f"  ğŸ¯ å¥–åŠ±ç»Ÿè®¡:")
        print(f"     - æœ€è¿‘100è½®å¹³å‡: {np.mean(recent_rewards):.2f}")
        print(f"     - æœ€è¿‘100è½®æ ‡å‡†å·®: {np.std(recent_rewards):.2f}")
        print(f"     - æœ€è¿‘100è½®æœ€å¤§: {np.max(recent_rewards):.2f}")
        print(f"     - æœ€è¿‘100è½®æœ€å°: {np.min(recent_rewards):.2f}")
        
        print(f"  ğŸ® è®­ç»ƒå‚æ•°:")
        print(f"     - Alpha (æ¸©åº¦): {self.sac_agent.alpha.item():.4f}")
        print(f"     - Buffer å¤§å°: {len(self.replay_buffer)}")
        
        print(f"  ğŸ† èƒœç‡ç»Ÿè®¡:")
        for opponent_type in ['basic', 'physics', 'mcts', 'self']:
            if self.game_counts[opponent_type] > 0:
                winrate = self.win_counts[opponent_type] / self.game_counts[opponent_type]
                print(f"     - vs {opponent_type:7s}: {winrate:5.1%} "
                      f"({self.win_counts[opponent_type]}/{self.game_counts[opponent_type]})")
        print("=" * 80 + "\n")
    
    def _evaluate(self):
        """è¯„ä¼°å½“å‰ç­–ç•¥"""
        # TODO: å®ç°è¯„ä¼°é€»è¾‘
        pass
    
    def _save_checkpoint(self):
        """ä¿å­˜checkpoint"""
        checkpoint_path = os.path.join(
            CHECKPOINT_CONFIG['save_dir'],
            f"checkpoint_ep{self.global_episode}.pth"
        )
        
        self.sac_agent.save(checkpoint_path)
        
        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        state_path = checkpoint_path.replace('.pth', '_state.pth')
        torch.save({
            'global_episode': self.global_episode,
            'current_stage': self.current_stage,
            'stage_episode': self.stage_episode,
            'episode_rewards': self.episode_rewards,
            'win_counts': self.win_counts,
            'game_counts': self.game_counts,
        }, state_path)
        
        # ä¿å­˜ replay buffer
        buffer_path = checkpoint_path.replace('.pth', '_buffer.pkl')
        self.replay_buffer.save(buffer_path)
        
        print(f"ğŸ’¾ Checkpoint å·²ä¿å­˜: {checkpoint_path}")
        print(f"ğŸ’¾ Buffer å·²ä¿å­˜: {len(self.replay_buffer)} transitions")
    
    def _load_checkpoint(self, checkpoint_path):
        """åŠ è½½checkpoint"""
        self.sac_agent.load(checkpoint_path)
        
        # åŠ è½½è®­ç»ƒçŠ¶æ€
        state_path = checkpoint_path.replace('.pth', '_state.pth')
        if os.path.exists(state_path):
            state = torch.load(state_path, map_location=DEVICE, weights_only=True)
            self.global_episode = state['global_episode']
            self.current_stage = state['current_stage']
            self.stage_episode = state['stage_episode']
            self.episode_rewards = state['episode_rewards']
            self.win_counts = state['win_counts']
            self.game_counts = state['game_counts']
            
            print(f"ğŸ“‹ è®­ç»ƒçŠ¶æ€å·²æ¢å¤: Episode {self.global_episode}, Stage {self.current_stage}")
        
        # åŠ è½½ replay buffer
        buffer_path = checkpoint_path.replace('.pth', '_buffer.pkl')
        self.replay_buffer.load(buffer_path)
        
        print(f"ğŸ“‚ Checkpoint å·²åŠ è½½: {checkpoint_path}")
    
    def _get_next_stage(self, current_stage):
        """è·å–ä¸‹ä¸€ä¸ªè®­ç»ƒé˜¶æ®µ"""
        stages = list(TRAINING_STAGES.keys())
        current_idx = stages.index(current_stage)
        if current_idx < len(stages) - 1:
            return stages[current_idx + 1]
        return None  # æ‰€æœ‰é˜¶æ®µå®Œæˆ


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    parser = argparse.ArgumentParser(description='å¹¶è¡Œ SAC è®­ç»ƒ')
    parser.add_argument('--resume', type=str, default=None,
                        help='ä» checkpoint æ¢å¤è®­ç»ƒ')
    args = parser.parse_args()
    
    try:
        trainer = ParallelSACTrainer(resume_from=args.resume)
        trainer.train()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        if 'trainer' in locals():
            trainer._save_checkpoint()
            print("æ¨¡å‹å·²ä¿å­˜")
    except Exception as e:
        print(f"\n\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        if 'trainer' in locals():
            trainer._save_checkpoint()
            print("æ¨¡å‹å·²ä¿å­˜")


if __name__ == '__main__':
    main()
