"""
Train SAC Parallel - å¹¶è¡Œ SAC è®­ç»ƒä¸»è„šæœ¬
å®ç°æ‰¹é‡å¹¶è¡Œå¯¹å±€ä»¥åŠ é€Ÿè®­ç»ƒ
"""

import os
import sys

# ==================== CUDA ç¯å¢ƒè®¾ç½® ====================
if 'CUDA_VISIBLE_DEVICES' not in os.environ:
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'

# é™åˆ¶CPUæ ¸å¿ƒæ•°ï¼ˆé¿å…è¿‡åº¦å¹¶è¡Œï¼‰
os.environ['OMP_NUM_THREADS'] = '16'
os.environ['MKL_NUM_THREADS'] = '16'
os.environ['OPENBLAS_NUM_THREADS'] = '16'
os.environ['NUMEXPR_NUM_THREADS'] = '16'

import time
import numpy as np
import torch
import torch.multiprocessing as mp
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed
from collections import defaultdict

# è®¾ç½® multiprocessing å¯åŠ¨æ–¹æ³•ï¼ˆCUDA å…¼å®¹ï¼‰
try:
    mp.set_start_method('spawn', force=True)
except RuntimeError:
    pass  # å·²ç»è®¾ç½®è¿‡äº†

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from config import (
    TRAINING_STAGES, EVAL_CONFIG, CHECKPOINT_CONFIG, LOG_CONFIG,
    DEVICE, SAC_CONFIG, denormalize_action
)
from state_encoder import StateEncoder
from reward_shaper import RewardShaper, get_ball_ids_by_type, count_remaining_balls
from sac_agent import SACAgent, SACAgentWrapper
from replay_buffer import ReplayBuffer, EpisodeTracker
from opponent_pool import OpponentPool
from poolenv import PoolEnv


# ==================== Worker Initialization (for multiprocessing) ====================

def _worker_init():
    """å­è¿›ç¨‹åˆå§‹åŒ– - æŠ‘åˆ¶æ‰€æœ‰æ‰“å°è¾“å‡º"""
    import sys
    import io
    # é‡å®šå‘ stdout å’Œ stderr åˆ°ç©ºè®¾å¤‡
    sys.stdout = io.StringIO()
    sys.stderr = io.StringIO()


# ==================== Worker Functions (for multiprocessing) ====================

def _warmup_worker(worker_id):
    """Warmup worker - åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œéšæœºç­–ç•¥episode"""
    # æ¯ä¸ªè¿›ç¨‹åˆ›å»ºè‡ªå·±çš„ç¯å¢ƒå’Œç»„ä»¶
    from opponent_pool import OpponentPool
    env = PoolEnv(verbose=False)
    env.enable_noise = False
    
    opponent_pool = OpponentPool(verbose=False)
    opponent = opponent_pool.get_opponent('random')
    state_encoder = StateEncoder()
    reward_shaper = RewardShaper()
    
    transitions = []
    env.reset(target_ball='solid')
    done = False
    
    while not done:
        current_player = env.get_curr_player()
        
        if current_player == 'A':  # SAC agent (éšæœºç­–ç•¥)
            state = state_encoder.encode_from_env(env, 'A')
            action = np.random.uniform(-1, 1, SAC_CONFIG['action_dim'])
            
            action_dict = denormalize_action(action)
            shot_result = env.take_shot(action_dict)
            
            my_type = env.player_targets['A'][0]
            enemy_type = 'stripe' if my_type == 'solid' else 'solid'
            my_balls_before = count_remaining_balls(
                env.balls, get_ball_ids_by_type(my_type)
            )
            enemy_balls_before = count_remaining_balls(
                env.balls, get_ball_ids_by_type(enemy_type)
            )
            
            reward = reward_shaper.calculate_immediate_reward(
                shot_result, my_balls_before, enemy_balls_before
            )
            
            next_state = state_encoder.encode_from_env(env, 'A')
            done = env.get_done()[0]
            
            transitions.append((state, action, reward, next_state, done))
        
        else:  # å¯¹æ‰‹
            balls, my_type, table = env.get_observation()
            action_dict = opponent.decision(balls, my_type, table)
            env.take_shot(action_dict)
            done = env.get_done()[0]
    
    # æ¸…ç†ç¯å¢ƒ
    env.close() if hasattr(env, 'close') else None
    return transitions


def _train_worker(worker_id, stage_config, actor_state_dict, checkpoint_pool_data):
    """Training worker - åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œè®­ç»ƒepisode"""
    # Windows ä¸æ”¯æŒ signal.alarmï¼Œä½¿ç”¨ threading.Timer ä½œä¸ºæ›¿ä»£
    import threading
    timeout_event = threading.Event()
    timeout_occurred = [False]  # ä½¿ç”¨åˆ—è¡¨é¿å…é—­åŒ…é—®é¢˜
    
    def check_timeout():
        timeout_occurred[0] = True
    
    timer = threading.Timer(600, check_timeout)  # 600ç§’è¶…æ—¶
    timer.daemon = True
    timer.start()
    
    try:
        # æ¯ä¸ªè¿›ç¨‹åˆ›å»ºè‡ªå·±çš„ç¯å¢ƒå’Œç»„ä»¶
        from opponent_pool import OpponentPool
        env = PoolEnv(verbose=False)
        env.enable_noise = False
        
        opponent_pool = OpponentPool(verbose=False)
        
        # æ¢å¤ checkpoint æ± ï¼ˆä»ä¸»è¿›ç¨‹ä¼ é€’ï¼‰
        for cp_data in checkpoint_pool_data:
            # ç›´æ¥æ·»åŠ åˆ°æ± ä¸­ï¼ˆä¸éœ€è¦å®Œæ•´çš„ agentï¼‰
            opponent_pool.checkpoint_pool.append(cp_data)
        
        state_encoder = StateEncoder()
        reward_shaper = RewardShaper()
        
        # åˆ›å»ºactorç½‘ç»œå¹¶åŠ è½½æƒé‡ï¼ˆåªç”¨äºæ¨ç†ï¼‰
        sac_agent = SACAgent()
        sac_agent.actor.load_state_dict(actor_state_dict)
        sac_agent.actor.eval()
        
        # é€‰æ‹©å¯¹æ‰‹
        opponent = opponent_pool.sample_opponent(stage_config)
        opponent_type = _identify_opponent_type_worker(opponent)
        
        transitions = []
        target_ball = 'solid' if np.random.rand() < 0.5 else 'stripe'
        env.reset(target_ball=target_ball)
        
        episode_reward = 0.0
        episode_length = 0
        done = False
        
        while not done:
            # æ£€æŸ¥è¶…æ—¶
            if timeout_occurred[0]:
                raise TimeoutError(f"Episode {worker_id} è¶…æ—¶ï¼ˆ>600ç§’ï¼‰")
            
            current_player = env.get_curr_player()
            
            if current_player == 'A':  # SAC agent
                state = state_encoder.encode_from_env(env, 'A')
                
                # ä½¿ç”¨actorç½‘ç»œé€‰æ‹©åŠ¨ä½œ
                with torch.no_grad():
                    action = sac_agent.select_action(state, deterministic=False)
                
                my_type = env.player_targets['A'][0]
                enemy_type = 'stripe' if my_type == 'solid' else 'solid'
                my_balls_before = count_remaining_balls(
                    env.balls, get_ball_ids_by_type(my_type)
                )
                enemy_balls_before = count_remaining_balls(
                    env.balls, get_ball_ids_by_type(enemy_type)
                )
                
                action_dict = denormalize_action(action)
                shot_result = env.take_shot(action_dict)
                
                next_state = state_encoder.encode_from_env(env, 'A')
                done = env.get_done()[0]
                
                # è®¡ç®—å¥–åŠ±ï¼ˆåŒ…å«ç»ˆå±€å¥–åŠ±ï¼‰
                if done:
                    winner = env.get_winner()
                    i_won = (winner == 'A')
                    # åˆ¤æ–­èƒœåˆ©åŸå› ï¼šæ£€æŸ¥æ˜¯å¦æ‰“è¿›é»‘å…«
                    win_reason = 'active' if shot_result.get('BLACK_BALL_INTO_POCKET', False) else 'passive'
                    reward = reward_shaper.calculate_immediate_reward(
                        shot_result, my_balls_before, enemy_balls_before,
                        game_done=True, i_won=i_won, win_reason=win_reason
                    )
                else:
                    reward = reward_shaper.calculate_immediate_reward(
                        shot_result, my_balls_before, enemy_balls_before
                    )
                
                transitions.append((state, action, reward, next_state, done))
                episode_reward += reward
                episode_length += 1
            
            else:  # å¯¹æ‰‹
                balls, my_type, table = env.get_observation()
                action_dict = opponent.decision(balls, my_type, table)
                env.take_shot(action_dict)
                done = env.get_done()[0]
        
        # æ£€æŸ¥èƒœè´Ÿ
        winner = env.get_winner()
        won = (winner == 'A')
        
        # æ¸…ç†ç¯å¢ƒ
        env.close() if hasattr(env, 'close') else None
        timer.cancel()
        
        return {
            'transitions': transitions,
            'reward': episode_reward,
            'length': episode_length,
            'opponent': opponent_type,
            'won': won
        }
    
    except TimeoutError:
        # è¶…æ—¶ï¼šè¿”å›ç©ºç»“æœ
        timer.cancel()
        if 'env' in locals():
            env.close() if hasattr(env, 'close') else None
        return {
            'transitions': [],
            'reward': -50.0,  # è¶…æ—¶æƒ©ç½š
            'length': 0,
            'opponent': opponent_type if 'opponent_type' in locals() else 'unknown',
            'won': False
        }
    except Exception as e:
        # å…¶ä»–é”™è¯¯
        timer.cancel()
        if 'env' in locals():
            env.close() if hasattr(env, 'close') else None
        raise e


def _identify_opponent_type_worker(opponent):
    """è¯†åˆ«å¯¹æ‰‹ç±»å‹ (workerç‰ˆæœ¬)"""
    class_name = opponent.__class__.__name__
    if 'Random' in class_name:
        return 'random'
    elif 'Basic' in class_name:
        return 'basic'
    elif 'Physics' in class_name:
        return 'physics'
    elif 'MCTS' in class_name:
        return 'mcts'
    elif 'SAC' in class_name or 'Wrapper' in class_name:
        return 'self'
    else:
        return 'unknown'


class ParallelSACTrainer:
    """å¹¶è¡Œ SAC è®­ç»ƒå™¨ - æ‰¹é‡è¿è¡Œå¤šä¸ªç¯å¢ƒå®ä¾‹"""
    
    def __init__(self, resume_from=None):
        """
        Args:
            resume_from: str, checkpoint è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºæ¢å¤è®­ç»ƒï¼‰
        """
        print("=" * 60)
        print("åˆå§‹åŒ–å¹¶è¡Œ SAC è®­ç»ƒå™¨")
        print("=" * 60)
        
        # åˆå§‹åŒ–ç»„ä»¶ï¼ˆå…±äº«ï¼‰
        self.state_encoder = StateEncoder()
        self.reward_shaper = RewardShaper()
        self.sac_agent = SACAgent()
        self.sac_wrapper = SACAgentWrapper(self.sac_agent, self.state_encoder)
        self.replay_buffer = ReplayBuffer(capacity=SAC_CONFIG['replay_buffer_size'])
        self.opponent_pool = OpponentPool()
        
        # å¹¶è¡Œé…ç½®
        self.num_parallel_envs = SAC_CONFIG['num_parallel_envs']
        self.update_frequency = SAC_CONFIG['update_frequency']
        
        # è®­ç»ƒçŠ¶æ€
        self.global_episode = 0
        self.current_stage = 'stage1'
        self.stage_episode = 0
        
        # ç»Ÿè®¡
        self.episode_rewards = []
        self.episode_lengths = []
        self.win_counts = {'basic': 0, 'physics': 0, 'mcts': 0, 'self': 0}
        self.game_counts = {'basic': 0, 'physics': 0, 'mcts': 0, 'self': 0}
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(CHECKPOINT_CONFIG['save_dir'], exist_ok=True)
        os.makedirs(LOG_CONFIG['log_dir'], exist_ok=True)
        
        # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if resume_from:
            self._load_checkpoint(resume_from)
        
        # åˆå§‹åŒ– checkpoint æ± ï¼ˆç”¨å½“å‰æ¨¡å‹å¡«å……ï¼Œæ”¯æŒæ—©æœŸè‡ªå¯¹å¼ˆï¼‰
        print("\nğŸ² åˆå§‹åŒ– checkpoint æ± ï¼ˆå½“å‰æ¨¡å‹ï¼‰...")
        for i in range(5):
            self.opponent_pool.add_checkpoint(
                self.sac_wrapper,
                episode=self.global_episode + i * -100,  # è´Ÿæ•°è¡¨ç¤ºåˆå§‹åŒ–
                metrics={'init': True, 'base_episode': self.global_episode}
            )
        print(f"âœ… Checkpoint æ± å·²åˆå§‹åŒ–: {len(self.opponent_pool.checkpoint_pool)} ä¸ª agent")
        
        print(f"âœ… åˆå§‹åŒ–å®Œæˆ")
        print(f"   Device: {DEVICE}")
        print(f"   å¹¶è¡Œç¯å¢ƒæ•°: {self.num_parallel_envs}")
        print(f"   æ›´æ–°é¢‘ç‡: æ¯ {self.update_frequency} episodes")
        print(f"   Replay Buffer Capacity: {SAC_CONFIG['replay_buffer_size']}")
        print(f"   Training Stages: {len(TRAINING_STAGES)}")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("\n" + "=" * 60)
        print("å¼€å§‹å¹¶è¡Œè®­ç»ƒ")
        print("=" * 60)
        
        # é¢„çƒ­é˜¶æ®µ
        if len(self.replay_buffer) < SAC_CONFIG['warmup_steps']:
            self._warmup_parallel()
        
        # éå†è®­ç»ƒé˜¶æ®µ
        for stage_name, stage_config in TRAINING_STAGES.items():
            if self.current_stage != stage_name:
                continue  # è·³è¿‡å·²å®Œæˆçš„é˜¶æ®µ
            
            self._train_stage(stage_name, stage_config)
            
            # å®Œæˆå½“å‰é˜¶æ®µï¼Œè¿›å…¥ä¸‹ä¸€é˜¶æ®µ
            self.current_stage = self._get_next_stage(stage_name)
            self.stage_episode = 0
        
        print("\n" + "=" * 60)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
    
    def _warmup_parallel(self):
        """å¹¶è¡Œé¢„çƒ­ - ä½¿ç”¨éšæœºç­–ç•¥å¡«å…… buffer"""
        print("\n" + "-" * 60)
        print(f"é¢„çƒ­é˜¶æ®µï¼šå¹¶è¡Œéšæœºç­–ç•¥å¡«å…… buffer åˆ° {SAC_CONFIG['warmup_steps']} ä¸ª transitions")
        print(f"   ç¯å¢ƒå™ªå£°: ç¦ç”¨")
        print(f"   å¯¹æ‰‹: RandomAgent (å¿«é€Ÿéšæœº)")
        print(f"   å¹¶è¡Œç¯å¢ƒæ•°: {self.num_parallel_envs}")
        print("-" * 60)
        
        while len(self.replay_buffer) < SAC_CONFIG['warmup_steps']:
            # å¹¶è¡Œè¿è¡Œå¤šä¸ªepisode
            batch_results = self._run_parallel_episodes_warmup(self.num_parallel_envs)
            
            # å­˜å‚¨æ‰€æœ‰transitions
            for transitions in batch_results:
                for transition in transitions:
                    self.replay_buffer.push(*transition)
            
            if len(self.replay_buffer) % 1000 == 0 or len(self.replay_buffer) >= SAC_CONFIG['warmup_steps']:
                progress = min(100.0, (len(self.replay_buffer) / SAC_CONFIG['warmup_steps']) * 100)
                print(f"  ğŸ“¦ Buffer: {len(self.replay_buffer):5d}/{SAC_CONFIG['warmup_steps']} [{progress:5.1f}%]")
        
        print(f"âœ… é¢„çƒ­å®Œæˆï¼Œbuffer size: {len(self.replay_buffer)}")
    
    def _run_parallel_episodes_warmup(self, num_episodes):
        """å¹¶è¡Œè¿è¡Œå¤šä¸ªwarmup episodes"""
        with ProcessPoolExecutor(max_workers=self.num_parallel_envs, initializer=_worker_init) as executor:
            futures = [
                executor.submit(_warmup_worker, i)
                for i in range(num_episodes)
            ]
            
            results = []
            for future in as_completed(futures):
                try:
                    transitions = future.result()
                    results.append(transitions)
                except Exception as e:
                    print(f"âŒ Warmup episode error: {e}")
                    import traceback
                    traceback.print_exc()
            
            return results
    
    def _run_single_episode_warmup(self):
        """è¿è¡Œå•ä¸ªwarmup episodeå¹¶è¿”å›transitions"""
        # æ¯ä¸ªçº¿ç¨‹åˆ›å»ºè‡ªå·±çš„ç¯å¢ƒå®ä¾‹
        env = PoolEnv(verbose=False)
        env.enable_noise = False
        opponent = self.opponent_pool.get_opponent('random')
        
        transitions = []
        env.reset(target_ball='solid')
        done = False
        
        while not done:
            current_player = env.get_curr_player()
            
            if current_player == 'A':  # SAC agent
                state = self.state_encoder.encode_from_env(env, 'A')
                action = np.random.uniform(-1, 1, SAC_CONFIG['action_dim'])
                
                action_dict = denormalize_action(action)
                shot_result = env.take_shot(action_dict)
                
                my_type = env.player_targets['A'][0]
                enemy_type = 'stripe' if my_type == 'solid' else 'solid'
                my_balls_before = count_remaining_balls(
                    env.balls, get_ball_ids_by_type(my_type)
                )
                enemy_balls_before = count_remaining_balls(
                    env.balls, get_ball_ids_by_type(enemy_type)
                )
                
                reward = self.reward_shaper.calculate_immediate_reward(
                    shot_result, my_balls_before, enemy_balls_before
                )
                
                next_state = self.state_encoder.encode_from_env(env, 'A')
                done = env.get_done()[0]
                
                transitions.append((state, action, reward, next_state, done))
            
            else:  # å¯¹æ‰‹
                balls, my_type, table = env.get_observation()
                action_dict = opponent.decision(balls, my_type, table)
                env.take_shot(action_dict)
                done = env.get_done()[0]
        
        return transitions
    
    def _train_stage(self, stage_name, stage_config):
        """è®­ç»ƒä¸€ä¸ªé˜¶æ®µ"""
        print("\n" + "=" * 80)
        print(f"ğŸ¯ é˜¶æ®µ {stage_name}: {stage_config['name']}")
        print("=" * 80)
        print(f"  ğŸ“ˆ ç›®æ ‡ Episodes: {stage_config['episodes']}")
        print(f"  ğŸ¤– å¯¹æ‰‹åˆ†å¸ƒ: {stage_config['opponents']}")
        print(f"  ğŸ† å®Œæˆæ¡ä»¶: {stage_config['target_metrics']}")
        print("=" * 80)
        print(f"Episode |     Stage Progress |                  Reward |   Steps |          Result")
        print("-" * 80)
        
        stage_start_episode = self.global_episode
        target_episodes = stage_config['episodes']
        
        # æ”¶é›†ä¸€æ‰¹episodesåå†æ›´æ–°
        episode_batch = []
        
        while self.stage_episode < target_episodes:
            # å¹¶è¡Œè¿è¡Œä¸€æ‰¹episodes
            print(f"ğŸ”„ å¼€å§‹è¿è¡Œ {self.update_frequency} ä¸ªå¹¶è¡Œ episodes...")
            batch_results = self._run_parallel_episodes_train(
                self.update_frequency,
                stage_config
            )
            print(f"âœ… å®Œæˆ {len(batch_results)} ä¸ª episodesï¼Œå¼€å§‹å¤„ç†ç»“æœ...")
            
            # å¤„ç†ç»“æœå¹¶å­˜å‚¨åˆ°buffer
            for ep_info in batch_results:
                # è·³è¿‡è¶…æ—¶çš„ episodeï¼ˆtransitions ä¸ºç©ºï¼‰
                if len(ep_info['transitions']) == 0:
                    print(f"âš ï¸  Episode {self.global_episode + 1} è¶…æ—¶ï¼Œè·³è¿‡")
                    continue
                
                self.global_episode += 1
                self.stage_episode += 1
                
                # æ›´æ–°ç»Ÿè®¡
                self.episode_rewards.append(ep_info['reward'])
                self.episode_lengths.append(ep_info['length'])
                self.game_counts[ep_info['opponent']] += 1
                if ep_info['won']:
                    self.win_counts[ep_info['opponent']] += 1
                
                # å­˜å‚¨transitions
                for transition in ep_info['transitions']:
                    self.replay_buffer.push(*transition)
                
                # æ‰“å°è¿›åº¦
                self._log_episode(ep_info, stage_name, target_episodes)
                
                # è¯¦ç»†ç»Ÿè®¡
                if self.global_episode % LOG_CONFIG['detailed_log_frequency'] == 0:
                    self._print_detailed_stats()
                
                # è¯„ä¼°
                if self.global_episode % EVAL_CONFIG['eval_frequency'] == 0:
                    self._evaluate()
                
                # ä¿å­˜checkpoint
                if self.global_episode % EVAL_CONFIG['checkpoint_frequency'] == 0:
                    self._save_checkpoint()
                    
                    # æ·»åŠ åˆ° self-play æ± ï¼ˆæ¯ 500 episodesï¼‰
                    if self.global_episode % 500 == 0:
                        self.opponent_pool.add_checkpoint(
                            self.sac_wrapper,
                            self.global_episode,
                            {'episode': self.global_episode, 'stage': stage_name}
                        )
            
            # æ‰¹é‡æ›´æ–°ç½‘ç»œï¼ˆæ”¶é›†å®Œä¸€æ‰¹episodesåï¼‰
            print(f"ğŸ”§ å¼€å§‹æ‰¹é‡æ›´æ–°ç½‘ç»œ...")
            self._batch_update_network(len(batch_results))
            print(f"âœ… ç½‘ç»œæ›´æ–°å®Œæˆ")
        
        print(f"\nâœ… é˜¶æ®µ {stage_name} å®Œæˆ")
    
    def _run_parallel_episodes_train(self, num_episodes, stage_config):
        """å¹¶è¡Œè¿è¡Œå¤šä¸ªè®­ç»ƒepisodes"""
        import time
        start_time = time.time()
        
        # å‡†å¤‡å…±äº«å‚æ•°ï¼ˆéœ€è¦åºåˆ—åŒ–ä¼ é€’ç»™å­è¿›ç¨‹ï¼‰
        actor_state = self.sac_agent.actor.state_dict()
        
        # ä¼ é€’ checkpoint æ± ä¿¡æ¯ç»™ worker
        checkpoint_pool_data = [
            {
                'episode': cp['episode'],
                'state_dict': cp['state_dict'],
            }
            for cp in self.opponent_pool.checkpoint_pool
        ]
        
        with ProcessPoolExecutor(max_workers=self.num_parallel_envs, initializer=_worker_init) as executor:
            futures = [
                executor.submit(_train_worker, i, stage_config, actor_state, checkpoint_pool_data)
                for i in range(num_episodes)
            ]
            
            results = []
            for future in as_completed(futures):
                try:
                    ep_info = future.result()
                    results.append(ep_info)
                except Exception as e:
                    print(f"âŒ Training episode error: {e}")
                    import traceback
                    traceback.print_exc()
            
            elapsed = time.time() - start_time
            print(f"   â±ï¸  è€—æ—¶: {elapsed:.2f}ç§’ (å¹³å‡ {elapsed/num_episodes:.2f}ç§’/episode)")
            
            return results
    
    def _run_single_episode_train(self, stage_config):
        """è¿è¡Œå•ä¸ªè®­ç»ƒepisode"""
        # æ¯ä¸ªçº¿ç¨‹åˆ›å»ºè‡ªå·±çš„ç¯å¢ƒå®ä¾‹
        env = PoolEnv(verbose=False)
        env.enable_noise = False
        
        # é€‰æ‹©å¯¹æ‰‹
        opponent = self.opponent_pool.sample_opponent(stage_config)
        opponent_type = self._identify_opponent_type(opponent)
        
        transitions = []
        target_ball = 'solid' if np.random.rand() < 0.5 else 'stripe'
        env.reset(target_ball=target_ball)
        
        episode_reward = 0.0
        episode_length = 0
        done = False
        
        while not done:
            current_player = env.get_curr_player()
            
            if current_player == 'A':  # SAC agent
                state = self.state_encoder.encode_from_env(env, 'A')
                
                # ä½¿ç”¨å…±äº«çš„SAC agentï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                with torch.no_grad():
                    action = self.sac_agent.select_action(state, deterministic=False)
                
                my_type = env.player_targets['A'][0]
                enemy_type = 'stripe' if my_type == 'solid' else 'solid'
                my_balls_before = count_remaining_balls(
                    env.balls, get_ball_ids_by_type(my_type)
                )
                enemy_balls_before = count_remaining_balls(
                    env.balls, get_ball_ids_by_type(enemy_type)
                )
                
                action_dict = denormalize_action(action)
                shot_result = env.take_shot(action_dict)
                
                next_state = self.state_encoder.encode_from_env(env, 'A')
                done = env.get_done()[0]
                
                # è®¡ç®—å¥–åŠ±ï¼ˆåŒ…å«ç»ˆå±€å¥–åŠ±ï¼‰
                if done:
                    winner = env.get_winner()
                    i_won = (winner == 'A')
                    # åˆ¤æ–­èƒœåˆ©åŸå› ï¼šæ£€æŸ¥æ˜¯å¦æ‰“è¿›é»‘å…«
                    win_reason = 'active' if shot_result.get('BLACK_BALL_INTO_POCKET', False) else 'passive'
                    reward = self.reward_shaper.calculate_immediate_reward(
                        shot_result, my_balls_before, enemy_balls_before,
                        game_done=True, i_won=i_won, win_reason=win_reason
                    )
                else:
                    reward = self.reward_shaper.calculate_immediate_reward(
                        shot_result, my_balls_before, enemy_balls_before
                    )
                
                transitions.append((state, action, reward, next_state, done))
                episode_reward += reward
                episode_length += 1
            
            else:  # å¯¹æ‰‹
                balls, my_type, table = env.get_observation()
                action_dict = opponent.decision(balls, my_type, table)
                env.take_shot(action_dict)
                done = env.get_done()[0]
        
        # æ£€æŸ¥èƒœè´Ÿ
        winner = env.get_winner()
        won = (winner == 'A')
        
        return {
            'transitions': transitions,
            'reward': episode_reward,
            'length': episode_length,
            'opponent': opponent_type,
            'won': won
        }
    
    def _batch_update_network(self, num_episodes):
        """æ‰¹é‡æ›´æ–°ç½‘ç»œ - åœ¨æ”¶é›†å®Œä¸€æ‰¹episodesåæ‰§è¡Œ"""
        if len(self.replay_buffer) < SAC_CONFIG['batch_size']:
            print(f"âš ï¸  Buffer ä¸è¶³ ({len(self.replay_buffer)} < {SAC_CONFIG['batch_size']})ï¼Œè·³è¿‡æ›´æ–°")
            return
        
        # ä¼˜åŒ–ï¼šå‡å°‘æ›´æ–°æ¬¡æ•°ï¼Œé¿å…å¡é¡¿
        # æ¯ä¸ª episode å¹³å‡ 1 æ¬¡æ›´æ–°å°±å¤Ÿäº†
        update_steps = num_episodes * SAC_CONFIG['gradient_steps']
        
        print(f"   æ›´æ–°æ­¥æ•°: {update_steps}")
        for i in range(update_steps):
            if i % 10 == 0:  # æ¯ 10 æ­¥æ‰“å°ä¸€æ¬¡è¿›åº¦
                print(f"   è¿›åº¦: {i}/{update_steps}", end='\r')
            # æ³¨æ„ï¼šupdate() æ–¹æ³•éœ€è¦ replay_buffer å’Œ batch_size ä½œä¸ºå‚æ•°
            self.sac_agent.update(self.replay_buffer, SAC_CONFIG['batch_size'])
        print(f"   è¿›åº¦: {update_steps}/{update_steps} âœ“")
    
    def _identify_opponent_type(self, opponent):
        """è¯†åˆ«å¯¹æ‰‹ç±»å‹"""
        class_name = opponent.__class__.__name__
        if 'Random' in class_name:
            return 'random'
        elif 'Basic' in class_name:
            return 'basic'
        elif 'Physics' in class_name:
            return 'physics'
        elif 'MCTS' in class_name:
            return 'mcts'
        elif 'SAC' in class_name or 'Wrapper' in class_name:
            return 'self'
        else:
            return 'unknown'
    
    def _log_episode(self, ep_info, stage_name, target_episodes):
        """è®°å½•episodeä¿¡æ¯"""
        if self.global_episode % LOG_CONFIG['log_frequency'] != 0:
            return
        
        progress = (self.stage_episode / target_episodes) * 100
        avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)
        result_symbol = 'âœ“' if ep_info['won'] else 'âœ—'
        
        print(f"Ep {self.global_episode:5d} | Stage {stage_name} [{progress:5.1f}%] | "
              f"Reward: {ep_info['reward']:7.2f} (avg100: {avg_reward:7.2f}) | "
              f"Steps: {ep_info['length']:2d} | {result_symbol} vs {ep_info['opponent']:7s}")
    
    def _print_detailed_stats(self):
        """æ‰“å°è¯¦ç»†ç»Ÿè®¡"""
        print("\n" + "=" * 80)
        print(f"ğŸ“Š è¯¦ç»†ç»Ÿè®¡ (Episode {self.global_episode})")
        print("=" * 80)
        
        recent_rewards = self.episode_rewards[-100:]
        print(f"  ğŸ¯ å¥–åŠ±ç»Ÿè®¡:")
        print(f"     - æœ€è¿‘100è½®å¹³å‡: {np.mean(recent_rewards):.2f}")
        print(f"     - æœ€è¿‘100è½®æ ‡å‡†å·®: {np.std(recent_rewards):.2f}")
        print(f"     - æœ€è¿‘100è½®æœ€å¤§: {np.max(recent_rewards):.2f}")
        print(f"     - æœ€è¿‘100è½®æœ€å°: {np.min(recent_rewards):.2f}")
        
        print(f"  ğŸ® è®­ç»ƒå‚æ•°:")
        print(f"     - Alpha (æ¸©åº¦): {self.sac_agent.alpha.item():.4f}")
        print(f"     - Buffer å¤§å°: {len(self.replay_buffer)}")
        
        print(f"  ğŸ† èƒœç‡ç»Ÿè®¡:")
        for opponent_type in ['basic', 'physics', 'mcts', 'self']:
            if self.game_counts[opponent_type] > 0:
                winrate = self.win_counts[opponent_type] / self.game_counts[opponent_type]
                print(f"     - vs {opponent_type:7s}: {winrate:5.1%} "
                      f"({self.win_counts[opponent_type]}/{self.game_counts[opponent_type]})")
        print("=" * 80 + "\n")
    
    def _evaluate(self):
        """è¯„ä¼°å½“å‰ç­–ç•¥"""
        # TODO: å®ç°è¯„ä¼°é€»è¾‘
        pass
    
    def _save_checkpoint(self):
        """ä¿å­˜checkpoint"""
        checkpoint_path = os.path.join(
            CHECKPOINT_CONFIG['save_dir'],
            f"checkpoint_ep{self.global_episode}.pth"
        )
        
        self.sac_agent.save(checkpoint_path)
        
        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        state_path = checkpoint_path.replace('.pth', '_state.pth')
        torch.save({
            'global_episode': self.global_episode,
            'current_stage': self.current_stage,
            'stage_episode': self.stage_episode,
            'episode_rewards': self.episode_rewards,
            'win_counts': self.win_counts,
            'game_counts': self.game_counts,
        }, state_path)
        
        # ä¿å­˜ replay buffer
        buffer_path = checkpoint_path.replace('.pth', '_buffer.pkl')
        self.replay_buffer.save(buffer_path)
        
        print(f"ğŸ’¾ Checkpoint å·²ä¿å­˜: {checkpoint_path}")
        print(f"ğŸ’¾ Buffer å·²ä¿å­˜: {len(self.replay_buffer)} transitions")
    
    def _load_checkpoint(self, checkpoint_path):
        """åŠ è½½checkpoint"""
        self.sac_agent.load(checkpoint_path)
        
        # åŠ è½½è®­ç»ƒçŠ¶æ€
        state_path = checkpoint_path.replace('.pth', '_state.pth')
        if os.path.exists(state_path):
            state = torch.load(state_path, map_location=DEVICE, weights_only=True)
            self.global_episode = state['global_episode']
            self.current_stage = state['current_stage']
            self.stage_episode = state['stage_episode']
            self.episode_rewards = state['episode_rewards']
            self.win_counts = state['win_counts']
            self.game_counts = state['game_counts']
            
            print(f"ğŸ“‹ è®­ç»ƒçŠ¶æ€å·²æ¢å¤: Episode {self.global_episode}, Stage {self.current_stage}")
        
        # åŠ è½½ replay buffer
        buffer_path = checkpoint_path.replace('.pth', '_buffer.pkl')
        self.replay_buffer.load(buffer_path)
        
        print(f"ğŸ“‚ Checkpoint å·²åŠ è½½: {checkpoint_path}")
    
    def _get_next_stage(self, current_stage):
        """è·å–ä¸‹ä¸€ä¸ªè®­ç»ƒé˜¶æ®µ"""
        stages = list(TRAINING_STAGES.keys())
        current_idx = stages.index(current_stage)
        if current_idx < len(stages) - 1:
            return stages[current_idx + 1]
        return None  # æ‰€æœ‰é˜¶æ®µå®Œæˆ


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    parser = argparse.ArgumentParser(description='å¹¶è¡Œ SAC è®­ç»ƒ')
    parser.add_argument('--resume', type=str, default=None,
                        help='ä» checkpoint æ¢å¤è®­ç»ƒ')
    args = parser.parse_args()
    
    try:
        trainer = ParallelSACTrainer(resume_from=args.resume)
        trainer.train()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        if 'trainer' in locals():
            trainer._save_checkpoint()
            print("æ¨¡å‹å·²ä¿å­˜")
    except Exception as e:
        print(f"\n\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        if 'trainer' in locals():
            trainer._save_checkpoint()
            print("æ¨¡å‹å·²ä¿å­˜")


if __name__ == '__main__':
    # multiprocessing éœ€è¦åœ¨ if __name__ == '__main__' ä¿æŠ¤ä¸‹è¿è¡Œ
    mp.freeze_support()  # Windows æ”¯æŒ
    
    # é™åˆ¶è¿›ç¨‹ä½¿ç”¨çš„CPUæ ¸å¿ƒï¼ˆå¯é€‰ï¼Œéœ€è¦ psutilï¼‰
    try:
        import psutil
        p = psutil.Process()
        # åªä½¿ç”¨å‰16ä¸ªCPUæ ¸å¿ƒï¼ˆ0-15ï¼‰
        available_cpus = list(range(min(16, psutil.cpu_count())))
        p.cpu_affinity(available_cpus)
        print(f"âœ… CPU äº²å’Œæ€§å·²è®¾ç½®: ä½¿ç”¨æ ¸å¿ƒ {available_cpus}")
    except ImportError:
        print("âš ï¸  psutil æœªå®‰è£…ï¼Œæ— æ³•è®¾ç½® CPU äº²å’Œæ€§ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰")
    except Exception as e:
        print(f"âš ï¸  è®¾ç½® CPU äº²å’Œæ€§å¤±è´¥: {e}")
    
    # æ£€æŸ¥ CUDA ç¯å¢ƒ
    print("="*60)
    print("ç¯å¢ƒæ£€æŸ¥")
    print("="*60)
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPU æ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
    print()
    
    main()
