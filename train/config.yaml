# SAC Self-Play Training Configuration
# =====================================

training:
  # 自对弈设置
  self_play: true
  defense_reward_enabled: false  # 后期可启用
  
  # 并行设置
  num_workers: 64              # 使用64个CPU核心（可通过 --workers 覆盖）
  games_per_batch: 64          # 每批游戏数（会自动根据worker数调整）
  learning_batch_size: 128     # 等待128局经验后学习（2批）
  
  # 经验回放（优先级缓冲区，保留高奖励经验）
  buffer_size: 1000000         # 100万经验
  batch_size: 2048             # GPU训练批量大小
  min_buffer_size: 20000       # 开始学习前的最小经验数
  priority_alpha: 0.6          # 优先级采样系数 [0,1]: 0=均匀随机, 1=完全按奖励优先级
  
  # SAC 超参数
  lr_actor: 3.0e-4
  lr_critic: 3.0e-4
  lr_alpha: 3.0e-4             # 自动温度调节
  gamma: 0.99                  # 折扣因子
  tau: 0.005                   # 目标网络软更新系数
  initial_alpha: 1.0           # 初始熵系数（增大以提高探索）
  target_entropy: null         # null表示自动计算 (-action_dim)
  
  # 训练控制
  total_episodes: 1000000      # 课程局数翻倍
  updates_per_batch: 8         # 每批经验后更新次数
  save_freq: 5000              # 每5000 episode保存
  eval_freq: 1000              # 每1000 episode评估
  log_freq: 100                # 每100 episode日志
  
  # 随机种子
  seed: 42

network:
  actor:
    # 加深网络：4层隐藏层，处理台球规则的隐含顺序性
    hidden_dims: [512, 512, 512, 256]
    activation: relu
  critic:
    hidden_dims: [512, 512, 512, 256]
    activation: relu

# 状态编码
state:
  # 编码器版本:
  #   v1: 基础编码 (64维) - 绝对坐标
  #   v2: 增强编码 (84维) - 相对坐标 + 进袋评分 + 路径检测
  encoder_version: 'v2'
  
  # 对称编码: 己方球统一编码，对方球统一编码
  symmetric_encoding: true

# 动作空间
action:
  # 动作空间类型:
  #   full: 5维完整动作 (V0, phi, theta, a, b)
  #   simple: 2维简化动作 (V0, phi) - 固定其他参数
  action_type: 'simple'
  
  # full 模式的参数范围
  ranges_full:
    V0: [0.5, 8.0]         # 初速度 m/s
    phi: [0.0, 360.0]      # 水平角度
    theta: [0.0, 45.0]     # 垂直角度
    a: [-0.4, 0.4]         # 横向偏移
    b: [-0.4, 0.4]         # 纵向偏移
  
  # simple 模式的参数范围（更保守）
  ranges_simple:
    V0: [1.0, 6.0]         # 限制速度范围
    phi: [0.0, 360.0]      # 水平角度
    # theta, a, b 固定为默认值

# 奖励权重
reward:
  w_terminal: 1.0
  w_pocket: 1.0
  w_position: 0.3
  w_foul: 1.0
  w_defense: 0.0           # 初期关闭

# 课程学习（移除只有黑八阶段，从1球开始学习正确的清台顺序）
# 总计100万局
curriculum:
  enabled: true
  stages:
    # 阶段1：入门环境（增加局数，让agent充分学习基础）
    - name: "stage_1"
      description: "入门：己方1球+对方1球+黑8（学习先打自己球再打黑八）"
      own_balls: 1
      enemy_balls: 1
      episodes: 120000
    
    # 阶段2：基础环境
    - name: "stage_2"
      description: "基础：己方2球+对方2球+黑8"
      own_balls: 2
      enemy_balls: 2
      episodes: 120000
    
    # 阶段3-4：中等环境
    - name: "stage_3"
      description: "进阶：己方3球+对方3球+黑8"
      own_balls: 3
      enemy_balls: 3
      episodes: 140000
    
    - name: "stage_4"
      description: "中等：己方4球+对方4球+黑8"
      own_balls: 4
      enemy_balls: 4
      episodes: 140000
    
    # 阶段5-6：较难环境
    - name: "stage_5"
      description: "挑战：己方5球+对方5球+黑8"
      own_balls: 5
      enemy_balls: 5
      episodes: 160000
    
    - name: "stage_6"
      description: "困难：己方6球+对方6球+黑8"
      own_balls: 6
      enemy_balls: 6
      episodes: 160000
    
    # 阶段7：完整环境
    - name: "stage_7"
      description: "完整环境：己方7球+对方7球+黑8"
      own_balls: 7
      enemy_balls: 7
      episodes: 160000

# 日志和保存
logging:
  log_dir: "logs/sac_selfplay"
  save_dir: "checkpoints/sac_selfplay"
  tensorboard: true
  wandb: false              # 可选wandb日志

